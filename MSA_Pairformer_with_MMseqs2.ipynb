{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoakiyama/MSA_Pairformer/blob/main/MSA_Pairformer_with_MMseqs2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MSA pairformer**"
      ],
      "metadata": {
        "id": "cp1lVXhoouvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Setup mmseqs2 + hhsuite + MSA pairformer\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "import gc\n",
        "import io\n",
        "import json\n",
        "import time\n",
        "import pickle\n",
        "import shutil\n",
        "import hashlib\n",
        "import tarfile\n",
        "import tempfile\n",
        "import warnings\n",
        "import importlib\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "from sys import version_info\n",
        "from contextlib import redirect_stdout, redirect_stderr\n",
        "from typing import List, Dict, Optional, Tuple, Union\n",
        "\n",
        "import requests\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class ColabFoldPairedMSA:\n",
        "    \"\"\"Simple class to get paired MSAs from ColabFold with extended filtering and genomic distance support\"\"\"\n",
        "    def __init__(self, host_url: str = \"https://api.colabfold.com\",\n",
        "                 cache_dir: Optional[str] = None):\n",
        "        self.host_url = host_url\n",
        "        self.job_id = None\n",
        "        self.parsed_entries = None  # List of parsed entries with metadata\n",
        "\n",
        "        # Set up cache directory\n",
        "        if cache_dir is None:\n",
        "            self.cache_dir = Path.home() / \".colabfold_cache\"\n",
        "        else:\n",
        "            self.cache_dir = Path(cache_dir)\n",
        "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        print(f\"Cache directory: {self.cache_dir}\")\n",
        "\n",
        "        # Initialize UniProt converter\n",
        "        self._init_uniprot_converter()\n",
        "\n",
        "    def _init_uniprot_converter(self):\n",
        "        \"\"\"Initialize UniProt ID to number conversion tables\"\"\"\n",
        "        from string import ascii_uppercase\n",
        "\n",
        "        # Initialize encoding tables\n",
        "        self.pa = {a: 0 for a in ascii_uppercase}\n",
        "        for a in [\"O\", \"P\", \"Q\"]:\n",
        "            self.pa[a] = 1\n",
        "\n",
        "        self.ma = [[{} for k in range(6)], [{} for k in range(6)]]\n",
        "\n",
        "        # Fill encoding tables\n",
        "        for n, t in enumerate(range(10)):\n",
        "            for i in [0, 1]:\n",
        "                for j in [0, 4]:\n",
        "                    self.ma[i][j][str(t)] = n\n",
        "\n",
        "        for n, t in enumerate(list(ascii_uppercase) + list(range(10))):\n",
        "            for i in [0, 1]:\n",
        "                for j in [1, 2]:\n",
        "                    self.ma[i][j][str(t)] = n\n",
        "            self.ma[1][3][str(t)] = n\n",
        "\n",
        "        for n, t in enumerate(ascii_uppercase):\n",
        "            self.ma[0][3][str(t)] = n\n",
        "            for i in [0, 1]:\n",
        "                self.ma[i][5][str(t)] = n\n",
        "\n",
        "    def _extract_uniprot_id(self, header: str) -> str:\n",
        "        \"\"\"Extract UniProt ID from header.\"\"\"\n",
        "        pos = header.find(\"UniRef\")\n",
        "        if pos == -1:\n",
        "            return \"\"\n",
        "\n",
        "        start = header.find('_', pos)\n",
        "        if start == -1:\n",
        "            return \"\"\n",
        "        start += 1\n",
        "\n",
        "        end = start\n",
        "        while end < len(header) and header[end] not in ' _\\t':\n",
        "            end += 1\n",
        "\n",
        "        uid = header[start:end]\n",
        "\n",
        "        # Validate - including UPI IDs\n",
        "        if len(uid) >= 3 and uid[:3] == \"UPI\":\n",
        "            return uid\n",
        "\n",
        "        # Regular UniProt ID validation\n",
        "        if len(uid) not in [6, 10]:\n",
        "            return \"\"\n",
        "        if not uid[0].isalpha():\n",
        "            return \"\"\n",
        "\n",
        "        return uid\n",
        "\n",
        "    def _uniprot_to_number(self, uniprot_ids: List[str]) -> List[int]:\n",
        "        \"\"\"Convert UniProt IDs to numbers for distance calculation.\"\"\"\n",
        "        numbers = []\n",
        "        for uni in uniprot_ids:\n",
        "            if not uni or not uni[0].isalpha():\n",
        "                numbers.append(0)\n",
        "                continue\n",
        "\n",
        "            p = self.pa.get(uni[0], 0)\n",
        "            tot, num = 1, 0\n",
        "\n",
        "            if len(uni) == 10:\n",
        "                for n, u in enumerate(reversed(uni[-4:])):\n",
        "                    if str(u) in self.ma[p][n]:\n",
        "                        num += self.ma[p][n][str(u)] * tot\n",
        "                        tot *= len(self.ma[p][n].keys())\n",
        "\n",
        "            for n, u in enumerate(reversed(uni[:6])):\n",
        "                if n < len(self.ma[p]) and str(u) in self.ma[p][n]:\n",
        "                    num += self.ma[p][n][str(u)] * tot\n",
        "                    tot *= len(self.ma[p][n].keys())\n",
        "\n",
        "            numbers.append(num)\n",
        "\n",
        "        return numbers\n",
        "\n",
        "    def _calculate_genomic_distances(self, entry: Dict) -> List[int]:\n",
        "        \"\"\"Calculate sequential distances between adjacent chains.\"\"\"\n",
        "        distances = []\n",
        "        nums = entry['uniprot_nums']\n",
        "\n",
        "        for i in range(1, len(nums)):\n",
        "            if nums[i-1] and nums[i]:  # Both must be valid numbers\n",
        "                dist = abs(nums[i] - nums[i-1])\n",
        "                distances.append(dist)\n",
        "            else:\n",
        "                distances.append(-1)  # Invalid distance\n",
        "\n",
        "        return distances\n",
        "\n",
        "    def _get_cache_key(self, sequences: List[str], genomic_distance: Optional[int],\n",
        "                       prefix: Optional[str]) -> str:\n",
        "        \"\"\"Generate a unique cache key for the request\"\"\"\n",
        "        # Always use genomic_distance=20 for caching to maximize reuse\n",
        "        cache_genomic_distance = 20 if genomic_distance is not None else None\n",
        "\n",
        "        # Create a deterministic string representation\n",
        "        cache_data = {\n",
        "            'sequences': sequences,\n",
        "            'genomic_distance': cache_genomic_distance,\n",
        "            'prefix': prefix,\n",
        "            'host_url': self.host_url\n",
        "        }\n",
        "        cache_str = json.dumps(cache_data, sort_keys=True)\n",
        "\n",
        "        # Generate hash\n",
        "        cache_hash = hashlib.sha256(cache_str.encode()).hexdigest()[:16]\n",
        "\n",
        "        # Create human-readable prefix\n",
        "        seq_info = f\"{len(sequences)}seq\"\n",
        "        if prefix:\n",
        "            seq_info += f\"_{prefix}\"\n",
        "\n",
        "        return f\"{seq_info}_{cache_hash}\"\n",
        "\n",
        "    def _load_from_cache(self, cache_key: str) -> bool:\n",
        "        \"\"\"Try to load parsed entries from cache\"\"\"\n",
        "        cache_file = self.cache_dir / f\"{cache_key}.pkl\"\n",
        "\n",
        "        if cache_file.exists():\n",
        "            try:\n",
        "                with open(cache_file, 'rb') as f:\n",
        "                    cache_data = pickle.load(f)\n",
        "\n",
        "                self.parsed_entries = cache_data['parsed_entries']\n",
        "                self.job_id = cache_data.get('job_id', f\"cached_{cache_key}\")\n",
        "\n",
        "                print(f\"Loaded from cache: {cache_key}\")\n",
        "                return True\n",
        "            except Exception as e:\n",
        "                print(f\"Cache load failed: {e}\")\n",
        "                return False\n",
        "\n",
        "        return False\n",
        "\n",
        "    def _save_to_cache(self, cache_key: str):\n",
        "        \"\"\"Save parsed entries to cache\"\"\"\n",
        "        cache_file = self.cache_dir / f\"{cache_key}.pkl\"\n",
        "\n",
        "        try:\n",
        "            cache_data = {\n",
        "                'parsed_entries': self.parsed_entries,\n",
        "                'job_id': self.job_id,\n",
        "                'timestamp': time.time()\n",
        "            }\n",
        "\n",
        "            with open(cache_file, 'wb') as f:\n",
        "                pickle.dump(cache_data, f)\n",
        "\n",
        "            print(f\"Saved to cache: {cache_key}\")\n",
        "\n",
        "            # Also save a human-readable info file\n",
        "            info_file = self.cache_dir / f\"{cache_key}_info.json\"\n",
        "            info_data = {\n",
        "                'job_id': self.job_id,\n",
        "                'num_entries': len(self.parsed_entries),\n",
        "                'num_chains': len(self.parsed_entries[0]['sequences']) if self.parsed_entries else 0,\n",
        "                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(cache_data['timestamp']))\n",
        "            }\n",
        "            with open(info_file, 'w') as f:\n",
        "                json.dump(info_data, f, indent=2)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Cache save failed: {e}\")\n",
        "\n",
        "    def clear_cache(self, older_than_days: Optional[int] = None):\n",
        "        \"\"\"Clear cache files, optionally only those older than specified days\"\"\"\n",
        "        import glob\n",
        "\n",
        "        cache_files = glob.glob(str(self.cache_dir / \"*.pkl\"))\n",
        "        removed = 0\n",
        "\n",
        "        for cache_file in cache_files:\n",
        "            if older_than_days is not None:\n",
        "                # Check age\n",
        "                age_days = (time.time() - os.path.getmtime(cache_file)) / (24 * 3600)\n",
        "                if age_days < older_than_days:\n",
        "                    continue\n",
        "\n",
        "            try:\n",
        "                os.remove(cache_file)\n",
        "                # Also remove info file if exists\n",
        "                info_file = cache_file.replace('.pkl', '_info.json')\n",
        "                if os.path.exists(info_file):\n",
        "                    os.remove(info_file)\n",
        "                removed += 1\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        print(f\"Removed {removed} cache files\")\n",
        "\n",
        "    def submit_or_load_from_cache(self,\n",
        "                                  sequences: List[str],\n",
        "                                  genomic_distance: Optional[int] = 20,\n",
        "                                  prefix: Optional[str] = None,\n",
        "                                  use_cache: bool = True) -> Tuple[str, bool]:\n",
        "        \"\"\"Submit sequences or load from cache if available\n",
        "\n",
        "        Always uses genomic_distance=20 for caching to maximize reuse.\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (job_id, from_cache) where from_cache indicates if data was loaded from cache\n",
        "        \"\"\"\n",
        "        # Always use distance=20 for caching\n",
        "        cache_genomic_distance = 20 if genomic_distance is not None else None\n",
        "\n",
        "        # Generate cache key\n",
        "        cache_key = self._get_cache_key(sequences, cache_genomic_distance, prefix)\n",
        "        self._current_cache_key = cache_key\n",
        "\n",
        "        # Try to load from cache\n",
        "        if use_cache and self._load_from_cache(cache_key):\n",
        "            return self.job_id, True\n",
        "\n",
        "        # If not in cache, submit normally with distance=20\n",
        "        self.submit(sequences, cache_genomic_distance, prefix)\n",
        "        return self.job_id, False\n",
        "\n",
        "    def submit(self,\n",
        "               sequences: List[str],\n",
        "               genomic_distance: Optional[int] = 20,\n",
        "               prefix: Optional[str] = None) -> str:\n",
        "        \"\"\"Submit sequences and return job ID\"\"\"\n",
        "        # Create query\n",
        "        query = \"\"\n",
        "        for i, seq in enumerate(sequences, start=101):\n",
        "            if prefix:\n",
        "                query += f\">{prefix}_{i}\\n{seq}\\n\"\n",
        "            else:\n",
        "                query += f\">{i}\\n{seq}\\n\"\n",
        "\n",
        "        # Determine mode based on number of sequences\n",
        "        if len(sequences) == 1:\n",
        "            # Single sequence - use regular MSA mode\n",
        "            mode = \"env\"\n",
        "            endpoint = \"ticket/msa\"\n",
        "        else:\n",
        "            # Multiple sequences - use pairing mode\n",
        "            if genomic_distance is None:\n",
        "                mode = \"paircomplete\"\n",
        "            else:\n",
        "                mode = f\"paircomplete-pairfilterprox_{genomic_distance}\"\n",
        "            endpoint = \"ticket/pair\"\n",
        "\n",
        "        response = requests.post(\n",
        "            f'{self.host_url}/{endpoint}',\n",
        "            data={'q': query, 'mode': mode},\n",
        "            timeout=30\n",
        "        )\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            raise Exception(f\"Failed to submit: {response.text}\")\n",
        "\n",
        "        self.job_id = response.json()['id']\n",
        "        print(f\"Job submitted: {self.job_id} with mode: {mode}\")\n",
        "        return self.job_id\n",
        "\n",
        "    def wait(self, check_interval: int = 5):\n",
        "        \"\"\"Wait for job completion\"\"\"\n",
        "        while True:\n",
        "            response = requests.get(f'{self.host_url}/ticket/{self.job_id}', timeout=30)\n",
        "            status = response.json().get('status', 'UNKNOWN')\n",
        "            print(f\"Status: {status}\")\n",
        "\n",
        "            if status == \"COMPLETE\":\n",
        "                break\n",
        "            elif status == \"ERROR\":\n",
        "                raise Exception(\"Job failed\")\n",
        "            time.sleep(check_interval)\n",
        "\n",
        "    def download_and_parse(self, output_dir: str = \"results\"):\n",
        "        \"\"\"Download results and parse the MSA\"\"\"\n",
        "        # Download\n",
        "        Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "        tar_path = os.path.join(output_dir, f\"{self.job_id}.tar.gz\")\n",
        "\n",
        "        response = requests.get(f'{self.host_url}/result/download/{self.job_id}', timeout=60)\n",
        "        with open(tar_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "        # Extract\n",
        "        with tarfile.open(tar_path) as tar:\n",
        "            tar.extractall(output_dir)\n",
        "\n",
        "        # Check if this is a paired MSA or single sequence MSA\n",
        "        pair_a3m = os.path.join(output_dir, 'pair.a3m')\n",
        "        if os.path.exists(pair_a3m):\n",
        "            # Paired MSA\n",
        "            self.parsed_entries = self._parse_paired_a3m(pair_a3m)\n",
        "        else:\n",
        "            # Single sequence - combine multiple MSA files\n",
        "            self.parsed_entries = self._parse_single_msas(output_dir)\n",
        "\n",
        "        # Save to cache if we have a cache key\n",
        "        if hasattr(self, '_current_cache_key'):\n",
        "            self._save_to_cache(self._current_cache_key)\n",
        "\n",
        "    def _parse_msa_lines(self, lines: List[str]) -> List[Dict]:\n",
        "        \"\"\"Parse MSA lines into structured entries with UniProt ID extraction\"\"\"\n",
        "        entries = []\n",
        "        i = 0\n",
        "        is_first = True\n",
        "\n",
        "        while i < len(lines):\n",
        "            line = lines[i].rstrip()\n",
        "\n",
        "            if line.startswith('>'):\n",
        "                header = line\n",
        "                seq_lines = []\n",
        "                i += 1\n",
        "\n",
        "                # Collect sequence lines\n",
        "                while i < len(lines) and not lines[i].startswith('>'):\n",
        "                    if lines[i].strip():\n",
        "                        seq_lines.append(lines[i].rstrip())\n",
        "                    i += 1\n",
        "\n",
        "                sequence = ''.join(seq_lines)\n",
        "\n",
        "                # Parse header\n",
        "                header_parts = header.split('\\t')\n",
        "                header_clean = header_parts[0].lstrip('>').replace('UniRef100_', '')\n",
        "\n",
        "                # Extract UniProt ID\n",
        "                uid = self._extract_uniprot_id(header)\n",
        "                has_uniref = \"UniRef\" in header\n",
        "                uniprot_num = 0\n",
        "\n",
        "                if uid:\n",
        "                    # Convert to number\n",
        "                    uniprot_nums = self._uniprot_to_number([uid])\n",
        "                    uniprot_num = uniprot_nums[0] if uniprot_nums else 0\n",
        "\n",
        "                # For query sequence\n",
        "                if is_first:\n",
        "                    coverage = 1.0\n",
        "                    identity = 1.0\n",
        "                    evalue = 0.0\n",
        "                    alnscore = float('inf')\n",
        "                    is_first = False\n",
        "                else:\n",
        "                    # Extract metadata from header\n",
        "                    coverage = None\n",
        "                    identity = None\n",
        "                    evalue = None\n",
        "                    alnscore = None\n",
        "\n",
        "                    if len(header_parts) >= 10:\n",
        "                        try:\n",
        "                            alnscore = float(header_parts[1])\n",
        "                            identity = float(header_parts[2])\n",
        "                            evalue = float(header_parts[3])\n",
        "                            q_start = int(header_parts[4])\n",
        "                            q_end = int(header_parts[5])\n",
        "                            q_len = int(header_parts[6])\n",
        "                            coverage = (q_end - q_start + 1) / q_len\n",
        "                        except:\n",
        "                            pass\n",
        "\n",
        "                    # Fallback values\n",
        "                    if coverage is None:\n",
        "                        coverage = 0.0\n",
        "                    if identity is None:\n",
        "                        identity = 0.0\n",
        "                    if evalue is None:\n",
        "                        evalue = float('inf')\n",
        "                    if alnscore is None:\n",
        "                        alnscore = 0.0\n",
        "\n",
        "                entries.append({\n",
        "                    'header': header_clean,\n",
        "                    'sequence': sequence,\n",
        "                    'coverage': coverage,\n",
        "                    'identity': identity,\n",
        "                    'evalue': evalue,\n",
        "                    'alnscore': alnscore,\n",
        "                    'uid': uid,\n",
        "                    'uniprot_num': uniprot_num,\n",
        "                    'has_uniref': has_uniref\n",
        "                })\n",
        "            else:\n",
        "                i += 1\n",
        "\n",
        "        return entries\n",
        "\n",
        "    def _parse_paired_a3m(self, a3m_path: str) -> List[Dict]:\n",
        "        \"\"\"Parse paired A3M file into list of entries with UniProt ID tracking\"\"\"\n",
        "        # First, separate MSAs by chain ID\n",
        "        raw_msas = {}\n",
        "        update_M = True\n",
        "        M = None\n",
        "\n",
        "        with open(a3m_path, 'r') as f:\n",
        "            for line in f:\n",
        "                if \"\\x00\" in line:\n",
        "                    line = line.replace(\"\\x00\", \"\")\n",
        "                    update_M = True\n",
        "\n",
        "                if line.startswith(\">\") and update_M:\n",
        "                    # Extract chain ID (101, 102, etc.)\n",
        "                    M = int(line[1:].rstrip().split('_')[-1])\n",
        "                    update_M = False\n",
        "                    if M not in raw_msas:\n",
        "                        raw_msas[M] = []\n",
        "\n",
        "                if M is not None:\n",
        "                    raw_msas[M].append(line.rstrip())\n",
        "\n",
        "        # Parse each chain's MSA\n",
        "        parsed_msas = {}\n",
        "        for seq_id, lines in raw_msas.items():\n",
        "            parsed_msas[seq_id] = self._parse_msa_lines(lines)\n",
        "\n",
        "        # Get sorted chain IDs\n",
        "        seq_ids = sorted(parsed_msas.keys())\n",
        "\n",
        "        # IMPORTANT: All chains should have the same number of sequences when paired\n",
        "        num_entries_per_chain = [len(parsed_msas[sid]) for sid in seq_ids]\n",
        "        if len(set(num_entries_per_chain)) > 1:\n",
        "            print(f\"Warning: Chains have different numbers of sequences: {dict(zip(seq_ids, num_entries_per_chain))}\")\n",
        "            print(\"Taking minimum to preserve pairing\")\n",
        "\n",
        "        min_entries = min(num_entries_per_chain)\n",
        "\n",
        "        # Stitch entries together - sequences at the same position are paired\n",
        "        stitched_entries = []\n",
        "        for i in range(min_entries):\n",
        "            # Collect info from each chain at position i\n",
        "            headers = []\n",
        "            sequences = []\n",
        "            coverages = []\n",
        "            identities = []\n",
        "            evalues = []\n",
        "            alnscores = []\n",
        "            uids = []\n",
        "            uniprot_nums = []\n",
        "            has_uniref = True  # Will be False if any chain doesn't have UniRef\n",
        "\n",
        "            for sid in seq_ids:\n",
        "                entry = parsed_msas[sid][i]\n",
        "                headers.append(entry['header'])\n",
        "                sequences.append(entry['sequence'])\n",
        "                coverages.append(entry['coverage'])\n",
        "                identities.append(entry['identity'])\n",
        "                evalues.append(entry['evalue'])\n",
        "                alnscores.append(entry['alnscore'])\n",
        "                uids.append(entry['uid'])\n",
        "                uniprot_nums.append(entry['uniprot_num'])\n",
        "                has_uniref = has_uniref and entry['has_uniref']\n",
        "\n",
        "            stitched_entries.append({\n",
        "                'headers': headers,\n",
        "                'sequences': sequences,\n",
        "                'coverages': coverages,\n",
        "                'identities': identities,\n",
        "                'evalues': evalues,\n",
        "                'alnscores': alnscores,\n",
        "                'uids': uids,\n",
        "                'uniprot_nums': uniprot_nums,\n",
        "                'has_uniref': has_uniref,\n",
        "                'is_query': (i == 0)\n",
        "            })\n",
        "\n",
        "        return stitched_entries\n",
        "\n",
        "    def _parse_single_msas(self, output_dir: str) -> List[Dict]:\n",
        "        \"\"\"Parse and combine single sequence MSAs\"\"\"\n",
        "        # MSA files to look for\n",
        "        msa_files = ['uniref.a3m', 'bfd.mgnify30.metaeuk30.smag30.a3m']\n",
        "\n",
        "        all_entries = []\n",
        "        seen_sequences = set()\n",
        "\n",
        "        for msa_file in msa_files:\n",
        "            msa_path = os.path.join(output_dir, msa_file)\n",
        "            if os.path.exists(msa_path):\n",
        "                with open(msa_path, 'r') as f:\n",
        "                    lines = f.readlines()\n",
        "\n",
        "                entries = self._parse_msa_lines(lines)\n",
        "\n",
        "                # Add non-duplicate entries\n",
        "                for entry in entries:\n",
        "                    if entry['sequence'] not in seen_sequences:\n",
        "                        seen_sequences.add(entry['sequence'])\n",
        "                        all_entries.append({\n",
        "                            'headers': [entry['header']],\n",
        "                            'sequences': [entry['sequence']],\n",
        "                            'coverages': [entry['coverage']],\n",
        "                            'identities': [entry['identity']],\n",
        "                            'evalues': [entry['evalue']],\n",
        "                            'alnscores': [entry['alnscore']],\n",
        "                            'uids': [entry['uid']],\n",
        "                            'uniprot_nums': [entry['uniprot_num']],\n",
        "                            'has_uniref': entry['has_uniref'],\n",
        "                            'is_query': len(all_entries) == 0\n",
        "                        })\n",
        "\n",
        "        return all_entries\n",
        "\n",
        "    def save_msa(self,\n",
        "                 output_file: str,\n",
        "                 min_coverage: Optional[float] = None,\n",
        "                 min_identity: Optional[float] = None,\n",
        "                 max_evalue: Optional[float] = None,\n",
        "                 min_alnscore: Optional[float] = None,\n",
        "                 max_genomic_distance: Optional[int] = None) -> Tuple[int, List[Dict]]:\n",
        "        \"\"\"Save MSA with optional filtering including genomic distance\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (number of sequences written, list of filtered entries)\n",
        "        \"\"\"\n",
        "        if not self.parsed_entries:\n",
        "            raise ValueError(\"No MSA loaded. Run download_and_parse first.\")\n",
        "\n",
        "        # Smart parsing: convert percentages to fractions\n",
        "        if min_coverage is not None and min_coverage > 1:\n",
        "            min_coverage = min_coverage / 100\n",
        "        if min_identity is not None and min_identity > 1:\n",
        "            min_identity = min_identity / 100\n",
        "\n",
        "        sequences_written = 0\n",
        "        sequences_filtered = 0\n",
        "        filtered_entries = []  # Store entries that pass the filter\n",
        "\n",
        "        num_chains = len(self.parsed_entries[0]['sequences']) if self.parsed_entries else 0\n",
        "\n",
        "        with open(output_file, 'w') as f:\n",
        "            for entry in self.parsed_entries:\n",
        "                # Skip filtering for query sequence\n",
        "                if not entry['is_query']:\n",
        "                    # All chains must pass the filter\n",
        "                    filter_reasons = []\n",
        "\n",
        "                    if min_coverage and any(c < min_coverage for c in entry['coverages']):\n",
        "                        filter_reasons.append(f\"coverage < {min_coverage}\")\n",
        "                    if min_identity and any(i < min_identity for i in entry['identities']):\n",
        "                        filter_reasons.append(f\"identity < {min_identity}\")\n",
        "                    if max_evalue is not None and any(e > max_evalue for e in entry['evalues'] if e is not None):\n",
        "                        filter_reasons.append(f\"evalue > {max_evalue}\")\n",
        "                    if min_alnscore is not None and any(a < min_alnscore for a in entry['alnscores'] if a is not None):\n",
        "                        filter_reasons.append(f\"alnscore < {min_alnscore}\")\n",
        "\n",
        "                    # Genomic distance filtering\n",
        "                    if max_genomic_distance is not None and entry['has_uniref']:\n",
        "                        distances = self._calculate_genomic_distances(entry)\n",
        "\n",
        "                        if num_chains == 2:\n",
        "                            # Simple case: check single distance\n",
        "                            if distances[0] != -1 and distances[0] > max_genomic_distance:\n",
        "                                filter_reasons.append(f\"genomic distance > {max_genomic_distance}\")\n",
        "                        else:\n",
        "                            # For >2 chains: check if all distances exceed threshold\n",
        "                            # (relaxed filtering - keep if ANY distance is within threshold)\n",
        "                            valid_distances = [d for d in distances if d != -1]\n",
        "                            if valid_distances and all(d > max_genomic_distance for d in valid_distances):\n",
        "                                filter_reasons.append(f\"all genomic distances > {max_genomic_distance}\")\n",
        "\n",
        "                    if filter_reasons:\n",
        "                        sequences_filtered += 1\n",
        "                        continue\n",
        "\n",
        "                # Write entry with modified header\n",
        "                if entry['is_query']:\n",
        "                    # Query header format: query_len1_len2_len3\n",
        "                    header = \"query\"\n",
        "                    for seq in entry['sequences']:\n",
        "                        header += f\"_len{len(seq)}\"\n",
        "                else:\n",
        "                    # Regular header format: UID1_UID2_UID3_dist1-2_dist2-3\n",
        "                    # First write UIDs (or original headers if no UID)\n",
        "                    header_parts = []\n",
        "                    for i, uid in enumerate(entry['uids']):\n",
        "                        if uid:\n",
        "                            header_parts.append(uid)\n",
        "                        else:\n",
        "                            header_parts.append(entry['headers'][i])\n",
        "\n",
        "                    header = '_'.join(header_parts)\n",
        "\n",
        "                    # Add distances if we have valid UIDs\n",
        "                    if entry['has_uniref'] and all(entry['uids']):\n",
        "                        distances = self._calculate_genomic_distances(entry)\n",
        "                        for dist in distances:\n",
        "                            if dist != -1:\n",
        "                                header += f\"_{dist}\"\n",
        "\n",
        "                sequence = ''.join(entry['sequences'])\n",
        "                f.write(f\">{header}\\n{sequence}\\n\")\n",
        "                sequences_written += 1\n",
        "                filtered_entries.append(entry)\n",
        "\n",
        "        print(f\"Saved {sequences_written} sequences to {output_file}\")\n",
        "        if sequences_filtered > 0:\n",
        "            print(f\"Filtered out {sequences_filtered} sequences\")\n",
        "        return sequences_written, filtered_entries\n",
        "\n",
        "    def get_stats(self, entries: Optional[List[Dict]] = None) -> Dict:\n",
        "        \"\"\"Get statistics about the MSA\n",
        "\n",
        "        Args:\n",
        "            entries: Optional list of entries to calculate stats from.\n",
        "                    If None, uses all parsed entries.\n",
        "        \"\"\"\n",
        "        if entries is None:\n",
        "            entries = self.parsed_entries\n",
        "\n",
        "        if not entries:\n",
        "            return {}\n",
        "\n",
        "        num_chains = len(entries[0]['sequences']) if entries else 0\n",
        "\n",
        "        stats = {\n",
        "            'num_chains': num_chains,\n",
        "            'num_entries': len(entries),\n",
        "        }\n",
        "\n",
        "        # Per-chain statistics\n",
        "        for i in range(num_chains):\n",
        "            coverages = [e['coverages'][i] for e in entries[1:]]  # Skip query\n",
        "            identities = [e['identities'][i] for e in entries[1:]]\n",
        "            evalues = [e['evalues'][i] for e in entries[1:] if e['evalues'][i] is not None]\n",
        "            alnscores = [e['alnscores'][i] for e in entries[1:] if e['alnscores'][i] is not None]\n",
        "\n",
        "            chain_id = i + 101\n",
        "            stats[f'chain_{chain_id}'] = {\n",
        "                'query_length': len(entries[0]['sequences'][i]) if entries else 0,\n",
        "                'avg_coverage': sum(coverages) / len(coverages) if coverages else 0,\n",
        "                'avg_identity': sum(identities) / len(identities) if identities else 0,\n",
        "                'avg_evalue': sum(evalues) / len(evalues) if evalues else 0,\n",
        "                'avg_alnscore': sum(alnscores) / len(alnscores) if alnscores else 0,\n",
        "                'min_evalue': min(evalues) if evalues else None,\n",
        "                'max_alnscore': max(alnscores) if alnscores else None,\n",
        "            }\n",
        "\n",
        "        return stats\n",
        "\n",
        "def get_paired_msa(sequences: Union[str, List[str]],\n",
        "                   output_file: str,\n",
        "                   genomic_distance: Optional[int] = 20,\n",
        "                   min_coverage: Optional[float] = None,\n",
        "                   min_identity: Optional[float] = None,\n",
        "                   max_evalue: Optional[float] = None,\n",
        "                   min_alnscore: Optional[float] = None,\n",
        "                   prefix: Optional[str] = None,\n",
        "                   host_url: str = \"https://api.colabfold.com\",\n",
        "                   cache_dir: Optional[str] = None,\n",
        "                   use_cache: bool = True,\n",
        "                   keep_temp: bool = False) -> str:\n",
        "    \"\"\"\n",
        "    Simple wrapper to get paired MSA from ColabFold with extended filtering\n",
        "\n",
        "    Args:\n",
        "        sequences: List of protein sequences or a single string with ':' delimiter\n",
        "        output_file: Path to save the stitched MSA\n",
        "        genomic_distance: Genomic distance for pairing (default: 20, None for no filtering)\n",
        "        min_coverage: Minimum coverage filter (0-1 or 0-100 for percentage)\n",
        "        min_identity: Minimum identity filter (0-1 or 0-100 for percentage)\n",
        "        max_evalue: Maximum e-value filter (e.g., 1e-5, 0.001)\n",
        "        min_alnscore: Minimum alignment score filter\n",
        "        prefix: Optional prefix for sequence IDs\n",
        "        host_url: ColabFold API URL\n",
        "        cache_dir: Directory for caching results (default: ~/.colabfold_cache)\n",
        "        use_cache: Whether to use caching (default: True)\n",
        "        keep_temp: If True, keep temporary files for debugging (default: False)\n",
        "\n",
        "    Returns:\n",
        "        Path to the output file\n",
        "    \"\"\"\n",
        "    # Handle string input\n",
        "    if isinstance(sequences, str):\n",
        "        # Split by ':' and clean up\n",
        "        sequences = [seq.strip() for seq in sequences.split(':') if seq.strip()]\n",
        "\n",
        "    # Further cleanup of sequences\n",
        "    cleaned_sequences = []\n",
        "    for seq in sequences:\n",
        "        # Remove whitespace and convert to uppercase\n",
        "        seq = ''.join(seq.split()).upper()\n",
        "        # Only add non-empty sequences\n",
        "        if seq:\n",
        "            cleaned_sequences.append(seq)\n",
        "\n",
        "    if not cleaned_sequences:\n",
        "        raise ValueError(\"No valid sequences provided\")\n",
        "\n",
        "    sequences = cleaned_sequences\n",
        "\n",
        "    # Store user's requested genomic distance\n",
        "    user_genomic_distance = genomic_distance\n",
        "\n",
        "    # Always fetch with distance=20 for better caching\n",
        "    fetch_genomic_distance = 20 if genomic_distance is not None else None\n",
        "\n",
        "    # Create temp directory\n",
        "    if keep_temp:\n",
        "        # Create a permanent temp directory\n",
        "        temp_dir = tempfile.mkdtemp(prefix=\"colabfold_\")\n",
        "        print(f\"Temporary files will be kept in: {temp_dir}\")\n",
        "    else:\n",
        "        # Use context manager for automatic cleanup\n",
        "        temp_context = tempfile.TemporaryDirectory()\n",
        "        temp_dir = temp_context.__enter__()\n",
        "\n",
        "    try:\n",
        "        # Create handler with cache support\n",
        "        msa = ColabFoldPairedMSA(host_url, cache_dir)\n",
        "\n",
        "        # Store cache key for later use - always use distance=20 for caching\n",
        "        cache_key = msa._get_cache_key(sequences, fetch_genomic_distance, prefix)\n",
        "        msa._current_cache_key = cache_key\n",
        "\n",
        "        # Submit or load from cache\n",
        "        job_id, from_cache = msa.submit_or_load_from_cache(sequences, fetch_genomic_distance, prefix, use_cache)\n",
        "\n",
        "        # Only wait and download if it's a new job (not from cache)\n",
        "        if not from_cache:\n",
        "            msa.wait()\n",
        "            # Download and parse\n",
        "            msa.download_and_parse(temp_dir)\n",
        "\n",
        "        # Save MSA to temporary file first with user's requested distance filtering\n",
        "        temp_output = os.path.join(temp_dir, \"temp_output.a3m\")\n",
        "        num_sequences, filtered_entries = msa.save_msa(\n",
        "            temp_output,\n",
        "            min_coverage,\n",
        "            min_identity,\n",
        "            max_evalue,\n",
        "            min_alnscore,\n",
        "            max_genomic_distance=user_genomic_distance  # THIS WAS MISSING!\n",
        "        )\n",
        "\n",
        "        # Move to final location\n",
        "        shutil.move(temp_output, output_file)\n",
        "\n",
        "        print(f\"\\nMSA saved to: {output_file}\")\n",
        "\n",
        "        # Print stats BEFORE filtering\n",
        "        stats_before = msa.get_stats()\n",
        "        print(f\"\\n=== Statistics BEFORE filtering ===\")\n",
        "        print(f\"Total entries: {stats_before['num_entries']}\")\n",
        "        for i in range(stats_before['num_chains']):\n",
        "            chain_stats = stats_before[f'chain_{i + 101}']\n",
        "            print(f\"Chain {i + 101}: query_length={chain_stats['query_length']}, \"\n",
        "                  f\"avg_coverage={chain_stats['avg_coverage']:.2f}, \"\n",
        "                  f\"avg_identity={chain_stats['avg_identity']:.2f}, \"\n",
        "                  f\"avg_evalue={chain_stats['avg_evalue']:.2e}, \"\n",
        "                  f\"avg_alnscore={chain_stats['avg_alnscore']:.1f}\")\n",
        "\n",
        "        # Print stats AFTER filtering (if any filtering was applied)\n",
        "        if any([min_coverage, min_identity, max_evalue is not None, min_alnscore is not None, user_genomic_distance != fetch_genomic_distance]):\n",
        "            stats_after = msa.get_stats(filtered_entries)\n",
        "            print(f\"\\n=== Statistics AFTER filtering ===\")\n",
        "            print(f\"Total entries: {stats_after['num_entries']} (saved)\")\n",
        "            for i in range(stats_after['num_chains']):\n",
        "                chain_stats = stats_after[f'chain_{i + 101}']\n",
        "                print(f\"Chain {i + 101}: query_length={chain_stats['query_length']}, \"\n",
        "                      f\"avg_coverage={chain_stats['avg_coverage']:.2f}, \"\n",
        "                      f\"avg_identity={chain_stats['avg_identity']:.2f}, \"\n",
        "                      f\"avg_evalue={chain_stats['avg_evalue']:.2e}, \"\n",
        "                      f\"avg_alnscore={chain_stats['avg_alnscore']:.1f}\")\n",
        "\n",
        "            # Print genomic distance filtering info if applied\n",
        "            if user_genomic_distance != fetch_genomic_distance:\n",
        "                print(f\"\\nGenomic distance filtering: {fetch_genomic_distance} → {user_genomic_distance}\")\n",
        "\n",
        "        if keep_temp:\n",
        "            print(f\"\\nTemporary files kept in: {temp_dir}\")\n",
        "            print(\"Files:\")\n",
        "            for file in os.listdir(temp_dir):\n",
        "                print(f\"  - {file}\")\n",
        "\n",
        "    finally:\n",
        "        # Clean up if not keeping temp files\n",
        "        if not keep_temp:\n",
        "            temp_context.__exit__(None, None, None)\n",
        "\n",
        "    return output_file\n",
        "\n",
        "\n",
        "def get_unique_jobname(base_jobname):\n",
        "    \"\"\"Get a unique jobname by incrementing if directory already exists.\"\"\"\n",
        "    if not os.path.exists(base_jobname):\n",
        "        return base_jobname\n",
        "\n",
        "    counter = 1\n",
        "    while os.path.exists(f\"{base_jobname}_{counter}\"):\n",
        "        counter += 1\n",
        "\n",
        "    return f\"{base_jobname}_{counter}\"\n",
        "\n",
        "def prepare_sequences(sequence, remove_duplicates = True):\n",
        "    \"\"\"\n",
        "    Clean and prepare sequences from input string.\n",
        "\n",
        "    Args:\n",
        "        sequence: Raw sequence string, chains separated by ':'\n",
        "        remove_duplicates: If True, removes duplicate sequences while preserving order\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (cleaned sequences, chain break indices)\n",
        "    \"\"\"\n",
        "    # Clean sequence\n",
        "    sequence = sequence.upper()\n",
        "    sequence = re.sub(\"[^A-Z:/()]\", \"\", sequence)\n",
        "    sequence = re.sub(\"\\(\", \":(\", sequence)\n",
        "    sequence = re.sub(\"\\)\", \"):\", sequence)\n",
        "    sequence = re.sub(\":+\", \":\", sequence)\n",
        "    sequence = re.sub(\"/+\", \"/\", sequence)\n",
        "    sequence = re.sub(\"^[:/]+\", \"\", sequence)\n",
        "    sequence = re.sub(\"[:/]+$\", \"\", sequence)\n",
        "\n",
        "    # Split into individual sequences\n",
        "    sequences = sequence.split(\":\")\n",
        "    sequences = [seq for seq in sequences if seq]\n",
        "\n",
        "    # Remove duplicates while preserving order\n",
        "    if remove_duplicates and len(sequences) > 1:\n",
        "        seen = set()\n",
        "        unique_sequences = []\n",
        "\n",
        "        for seq in sequences:\n",
        "            if seq not in seen:\n",
        "                seen.add(seq)\n",
        "                unique_sequences.append(seq)\n",
        "\n",
        "        if len(unique_sequences) < len(sequences):\n",
        "            print(f\"Note: Removed {len(sequences) - len(unique_sequences)} duplicate sequence(s)\")\n",
        "\n",
        "        sequences = unique_sequences\n",
        "\n",
        "    # Calculate chain break indices for the final sequences\n",
        "    chain_breaks = []\n",
        "    position = 0\n",
        "    for i, seq in enumerate(sequences[:-1]):  # All except last sequence\n",
        "        position += len(seq)\n",
        "        chain_breaks.append(position)\n",
        "\n",
        "    return sequences, chain_breaks\n",
        "\n",
        "def _setup_tools():\n",
        "  \"\"\"Download and compile C++ tools.\"\"\"\n",
        "\n",
        "  # Install HHsuite\n",
        "  hhsuite_path = \"hhsuite\"\n",
        "  if not os.path.isdir(hhsuite_path):\n",
        "      print(\"Installing HHsuite...\")\n",
        "      os.makedirs(hhsuite_path, exist_ok=True)\n",
        "      url = \"https://github.com/soedinglab/hh-suite/releases/download/v3.3.0/hhsuite-3.3.0-SSE2-Linux.tar.gz\"\n",
        "      os.system(f\"curl -fsSL {url} | tar xz -C {hhsuite_path}/\")\n",
        "\n",
        "  os.environ['PATH'] += f\":{hhsuite_path}/bin:{hhsuite_path}/scripts\"\n",
        "\n",
        "python_version = f\"{version_info.major}.{version_info.minor}\"\n",
        "\n",
        "# this part might not be needed\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "\n",
        "# Suppress progress bars and warnings\n",
        "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\".*torch.distributed.reduce_op.*\")\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*HF_TOKEN.*\")\n",
        "\n",
        "# Install MSA Pairformer (only if not already installed)\n",
        "if not os.path.isdir(\"MSA_Pairformer\"):\n",
        "    print(\"Setting up MSA Pairformer...\")\n",
        "\n",
        "    # Capture output for git clone\n",
        "    GIT_REPO = 'https://github.com/yoakiyama/MSA_Pairformer'\n",
        "    TMP_DIR = \"tmp\"\n",
        "    os.makedirs(TMP_DIR, exist_ok=True)\n",
        "\n",
        "    result = subprocess.run(\n",
        "        f\"git clone {GIT_REPO}.git\",\n",
        "        shell=True,\n",
        "        capture_output=True,\n",
        "        text=True\n",
        "    )\n",
        "\n",
        "    # Capture pip install output\n",
        "    with io.StringIO() as buf, redirect_stdout(buf), redirect_stderr(buf):\n",
        "        subprocess.run(\n",
        "            [\"pip\", \"install\", \"-e\", \"MSA_Pairformer/\", \"--no-deps\"],\n",
        "            capture_output=True,\n",
        "            text=True\n",
        "        )\n",
        "        subprocess.run(\n",
        "            [\"pip\", \"install\", \"biopython\", \"einx\", \"jaxtyping\"],\n",
        "            capture_output=True,\n",
        "            text=True\n",
        "        )\n",
        "\n",
        "    importlib.invalidate_caches()\n",
        "    # Add the package to Python path\n",
        "    package_path = os.path.abspath(\"MSA_Pairformer\")\n",
        "    if package_path not in sys.path:\n",
        "        sys.path.insert(0, package_path)\n",
        "\n",
        "    print(\"✓ MSA Pairformer installed successfully\")\n",
        "\n",
        "# Import MSA Pairformer modules\n",
        "from MSA_Pairformer.model import MSAPairformer\n",
        "from MSA_Pairformer.dataset import MSA, aa2tok_d, prepare_msa_masks\n",
        "\n",
        "# Initialize device\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "device_name = torch.cuda.get_device_name(device) if device.type == 'cuda' else 'CPU'\n",
        "print(f\"Using device: {device_name}\")\n",
        "\n",
        "# Load model ONCE and store globally\n",
        "if 'global_model' not in globals():\n",
        "    print(\"Loading MSA Pairformer model (this will only happen once)...\")\n",
        "\n",
        "    # Suppress HuggingFace warnings during model loading\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.filterwarnings(\"ignore\")\n",
        "        # Optionally capture stdout/stderr if the model loading is still too verbose\n",
        "        with io.StringIO() as buf, redirect_stderr(buf):\n",
        "            global_model = MSAPairformer.from_pretrained(device=device).to(torch.bfloat16)\n",
        "\n",
        "    print(\"✓ Model loaded successfully and cached for reuse!\")\n",
        "else:\n",
        "    print(\"✓ Using cached model\")\n",
        "\n",
        "_setup_tools()"
      ],
      "metadata": {
        "id": "zQKGgfwv2nfy",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title input sequence(s)\n",
        "sequence_a = \"PIAQIHILEGRSDEQKETLIREVSEAISRSLDAPLTSVRVIITEMAKGHFGIGGELASK\" # @param {\"type\":\"string\"}\n",
        "sequence_b = \"\" # @param {\"type\":\"string\"}\n",
        "sequence_c = \"\" # @param {\"type\":\"string\"}\n",
        "\n",
        "sequence = f\"{sequence_a}:{sequence_b}:{sequence_c}\"\n",
        "\n",
        "sequences, breaks = prepare_sequences(sequence)\n",
        "print(\"lengths\",[len(x) for x in sequences])\n",
        "\n",
        "#@markdown ----\n",
        "#@markdown **MSA options**\n",
        "\n",
        "cov = 75 #@param [\"0\", \"25\", \"50\", \"75\", \"90\", \"99\"] {type:\"raw\"}\n",
        "qid = 15 #@param [\"0\", \"15\", \"20\", \"30\", \"40\"] {type:\"raw\"}\n",
        "#@markdown For MSA Pairformer analyses, we typically recommend starting with\n",
        "#@markdown 75% coverage (cov), and 15% minimum sequence identity with query (qid).\n",
        "\n",
        "#@markdown ----\n",
        "#@markdown **Multimer settings (experimental option)**\n",
        "neighbor_stitching = True #@param {type:\"boolean\"}\n",
        "Δgene = 5 #@param [\"0\", \"1\", \"5\", \"10\", \"20\"] {type:\"raw\"}\n",
        "#@markdown For prokaryotes, it's sometimes helpful to stitch genes based on how far part the genes are on the genome.\n",
        "\n",
        "jobname = get_unique_jobname(\"tmp\")\n",
        "msa_file = f\"{jobname}.a3m\"\n",
        "get_paired_msa(\n",
        "    sequences,\n",
        "    msa_file,\n",
        "    min_coverage=cov,\n",
        "    min_identity=qid,\n",
        "    genomic_distance=Δgene if neighbor_stitching else None,  # This passes the user's selected distance\n",
        ")\n",
        "print(f\"MSA saved to: {msa_file}\")"
      ],
      "metadata": {
        "id": "SjH7HTACW1IW",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Generate embeddings and predict contacts using MSA Pairformer\n",
        "max_msa_depth = 512 # @param [\"64\",\"128\",\"256\",\"512\",\"1024\"] {\"type\":\"raw\"}\n",
        "\n",
        "import pandas as pd\n",
        "import bokeh.plotting\n",
        "bokeh.io.output_notebook()\n",
        "from bokeh.models import BasicTicker, PrintfTickFormatter\n",
        "from bokeh.palettes import viridis, RdBu\n",
        "from bokeh.transform import linear_cmap\n",
        "from bokeh.plotting import figure, show\n",
        "from matplotlib.colors import to_hex\n",
        "cmap = plt.colormaps[\"gray_r\"]\n",
        "gray = [to_hex(cmap(i)) for i in np.linspace(0, 1, 256)]\n",
        "\n",
        "\n",
        "def convert_to_numpy(obj):\n",
        "    \"\"\"\n",
        "    Recursively convert PyTorch tensors to numpy arrays, handling BFloat16 and nested structures.\n",
        "    \"\"\"\n",
        "    if isinstance(obj, torch.Tensor):\n",
        "        # Convert BFloat16 to Float32 first, then to numpy\n",
        "        if obj.dtype == torch.bfloat16:\n",
        "            return obj.float().cpu().numpy()\n",
        "        else:\n",
        "            return obj.cpu().numpy()\n",
        "    elif isinstance(obj, dict):\n",
        "        # Recursively convert dictionary values\n",
        "        return {key: convert_to_numpy(value) for key, value in obj.items()}\n",
        "    elif isinstance(obj, list):\n",
        "        # Recursively convert list elements\n",
        "        return [convert_to_numpy(item) for item in obj]\n",
        "    elif isinstance(obj, tuple):\n",
        "        # Recursively convert tuple elements\n",
        "        return tuple(convert_to_numpy(item) for item in obj)\n",
        "    else:\n",
        "        # Return as-is for non-tensor types\n",
        "        return obj\n",
        "\n",
        "def clear_gpu_memory(keep_model=True):\n",
        "    \"\"\"\n",
        "    Clear GPU memory while optionally keeping the model.\n",
        "    \"\"\"\n",
        "    # Get all objects in memory\n",
        "    for obj in gc.get_objects():\n",
        "        try:\n",
        "            if isinstance(obj, torch.Tensor):\n",
        "                # Skip model parameters if we want to keep the model\n",
        "                if keep_model and hasattr(obj, '_base') and obj._base is not None:\n",
        "                    continue\n",
        "                del obj\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # Multiple rounds of garbage collection\n",
        "    for _ in range(3):\n",
        "        gc.collect()\n",
        "\n",
        "    # Clear CUDA cache\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "def run_msa_pairformer(msa_file, breaks, total_length, max_msa_depth=512):\n",
        "    \"\"\"\n",
        "    Run MSA Pairformer using the pre-loaded global model.\n",
        "    Returns results as numpy arrays on CPU and cleans up everything except the model.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Clear memory before starting (but keep the model)\n",
        "        clear_gpu_memory(keep_model=True)\n",
        "\n",
        "        np.random.seed(42)\n",
        "        msa_obj = MSA(\n",
        "            msa_file_path=msa_file,\n",
        "            max_seqs=max_msa_depth,\n",
        "            max_length=total_length,\n",
        "            max_tokens=1e12,\n",
        "            diverse_select_method=\"hhfilter\",\n",
        "            hhfilter_kwargs={\"binary\": \"hhfilter\"}\n",
        "        )\n",
        "\n",
        "        msa_tokenized_t = msa_obj.diverse_tokenized_msa\n",
        "        msa_onehot_t = torch.nn.functional.one_hot(msa_tokenized_t, num_classes=len(aa2tok_d)).unsqueeze(0).float().to(device)\n",
        "        mask, msa_mask, full_mask, pairwise_mask = prepare_msa_masks(msa_obj.diverse_tokenized_msa.unsqueeze(0))\n",
        "        mask, msa_mask, full_mask, pairwise_mask = mask.to(device), msa_mask.to(device), full_mask.to(device), pairwise_mask.to(device)\n",
        "\n",
        "        # Run model inference\n",
        "        with torch.no_grad():\n",
        "            with torch.amp.autocast(dtype=torch.bfloat16, device_type=\"cuda\"):\n",
        "                res = global_model(  # Use the pre-loaded global model\n",
        "                    msa=msa_onehot_t.to(torch.bfloat16),\n",
        "                    mask=mask,\n",
        "                    msa_mask=msa_mask,\n",
        "                    full_mask=full_mask,\n",
        "                    pairwise_mask=pairwise_mask,\n",
        "                    complex_chain_break_indices=[breaks],\n",
        "                    return_seq_weights=True,\n",
        "                    return_pairwise_repr_layer_idx=None,\n",
        "                    return_msa_repr_layer_idx=None\n",
        "                )\n",
        "\n",
        "        # Convert results to numpy arrays on CPU\n",
        "        res_numpy = convert_to_numpy(res)\n",
        "\n",
        "        # Store additional info\n",
        "        res_numpy['total_length'] = total_length\n",
        "        res_numpy['max_msa_depth'] = max_msa_depth\n",
        "        res_numpy['weight_scale'] = msa_onehot_t.shape[1]\n",
        "\n",
        "        # Clean up everything except the model and results\n",
        "        del res, msa_onehot_t, mask, msa_mask, full_mask, pairwise_mask, msa_tokenized_t, msa_obj\n",
        "\n",
        "        return res_numpy\n",
        "\n",
        "    finally:\n",
        "        # Final cleanup (keeping the model)\n",
        "        clear_gpu_memory(keep_model=True)\n",
        "\n",
        "def contact_to_dataframe(con):\n",
        "  sequence_length = con.shape[0]\n",
        "  idx = [str(i) for i in np.arange(1, sequence_length + 1)]\n",
        "  df = pd.DataFrame(con, index=idx, columns=idx)\n",
        "  df = df.stack().reset_index()\n",
        "  df.columns = ['i', 'j', 'value']\n",
        "  return df\n",
        "\n",
        "\n",
        "results = run_msa_pairformer(msa_file, breaks,\n",
        "                             total_length=len(sequence.replace(\":\",\"\").replace(\" \",\"\")),\n",
        "                             max_msa_depth=max_msa_depth)\n",
        "\n",
        "clean_sequence = sequence.replace(\":\",\"\").replace(\" \",\"\")\n",
        "\n",
        "df = contact_to_dataframe(results[\"contacts\"][0])\n",
        "TOOLS = \"hover,save,pan,box_zoom,reset,wheel_zoom\"\n",
        "p = figure(title=\"COEVOLUTION\",\n",
        "          x_range=[str(x) for x in range(1,len(clean_sequence)+1)],\n",
        "          y_range=[str(x) for x in range(1,len(clean_sequence)+1)][::-1],\n",
        "          width=800, height=800,\n",
        "          tools=TOOLS, toolbar_location='below',\n",
        "          tooltips=[('i', '@i'), ('j', '@j'), ('value', '@value')])\n",
        "\n",
        "r = p.rect(x=\"i\", y=\"j\", width=1, height=1, source=df,\n",
        "          fill_color=linear_cmap('value', gray, low=df.value.min(), high=df.value.max()),\n",
        "          line_color=None)\n",
        "p.xaxis.visible = False  # Hide the x-axis\n",
        "p.yaxis.visible = False  # Hide the x-axis\n",
        "show(p)"
      ],
      "metadata": {
        "id": "wGsYwNc0nqmj",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ##show table of top covarying positions\n",
        "from google.colab import data_table\n",
        "\n",
        "sub_df = df[df[\"j\"]>df[\"i\"]].sort_values('value',ascending=False)\n",
        "data_table.DataTable(sub_df, include_index=False, num_rows_per_page=20, min_width=10)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "m0d3hL-ZtLsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualize sequence weights\n",
        "mean_seq_weights_a = np.mean(np.stack([results['seq_weights_list_d'][f\"layer_{layer_idx}\"][0] for layer_idx in range(16)]), axis=0)\n",
        "mean_seq_weights_a *= results[\"weight_scale\"]\n",
        "f, ax = plt.subplots(1, 1, figsize=(8,4))\n",
        "_ = ax.hist(mean_seq_weights_a, bins=50)\n",
        "ax.axvline(x=1, linestyle='--', color='red')\n",
        "ax.set_title(\"Sequence weight distribution\", size=18)\n",
        "ax.set_xlabel(\"Normalized sequence weight\", size=16)\n",
        "ax.set_ylabel(\"Count\", size=16)\n",
        "ax.tick_params(axis='both', which='major', labelsize=12)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "n_Wa6SZfp1YT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}