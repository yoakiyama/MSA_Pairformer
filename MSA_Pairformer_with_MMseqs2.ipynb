{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoakiyama/MSA_Pairformer/blob/dev-branch/MSA_Pairformer_with_MMseqs2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cp1lVXhoouvl"
      },
      "source": [
        "# **MSA Pairformer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQKGgfwv2nfy"
      },
      "outputs": [],
      "source": [
        "# @title Setup mmseqs2 + hhsuite + MSA pairformer\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "import gc\n",
        "import io\n",
        "import json\n",
        "import time\n",
        "import pickle\n",
        "import shutil\n",
        "import hashlib\n",
        "import tarfile\n",
        "import tempfile\n",
        "import warnings\n",
        "import importlib\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "from sys import version_info\n",
        "from contextlib import redirect_stdout, redirect_stderr\n",
        "from typing import List, Dict, Optional, Tuple, Union\n",
        "from google.colab import files\n",
        "\n",
        "import requests\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class ColabFoldPairedMSA:\n",
        "    \"\"\"Simple class to get paired MSAs from ColabFold with extended filtering and genomic distance support\"\"\"\n",
        "    def __init__(self, host_url: str = \"https://api.colabfold.com\",\n",
        "                 cache_dir: Optional[str] = None):\n",
        "        self.host_url = host_url\n",
        "        self.job_id = None\n",
        "        self.parsed_entries = None  # List of parsed entries with metadata\n",
        "\n",
        "        # Set up cache directory\n",
        "        if cache_dir is None:\n",
        "            self.cache_dir = Path.home() / \".colabfold_cache\"\n",
        "        else:\n",
        "            self.cache_dir = Path(cache_dir)\n",
        "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        print(f\"Cache directory: {self.cache_dir}\")\n",
        "\n",
        "        # Initialize UniProt converter\n",
        "        self._init_uniprot_converter()\n",
        "\n",
        "    def _init_uniprot_converter(self):\n",
        "        \"\"\"Initialize UniProt ID to number conversion tables\"\"\"\n",
        "        from string import ascii_uppercase\n",
        "\n",
        "        # Initialize encoding tables\n",
        "        self.pa = {a: 0 for a in ascii_uppercase}\n",
        "        for a in [\"O\", \"P\", \"Q\"]:\n",
        "            self.pa[a] = 1\n",
        "\n",
        "        self.ma = [[{} for k in range(6)], [{} for k in range(6)]]\n",
        "\n",
        "        # Fill encoding tables\n",
        "        for n, t in enumerate(range(10)):\n",
        "            for i in [0, 1]:\n",
        "                for j in [0, 4]:\n",
        "                    self.ma[i][j][str(t)] = n\n",
        "\n",
        "        for n, t in enumerate(list(ascii_uppercase) + list(range(10))):\n",
        "            for i in [0, 1]:\n",
        "                for j in [1, 2]:\n",
        "                    self.ma[i][j][str(t)] = n\n",
        "            self.ma[1][3][str(t)] = n\n",
        "\n",
        "        for n, t in enumerate(ascii_uppercase):\n",
        "            self.ma[0][3][str(t)] = n\n",
        "            for i in [0, 1]:\n",
        "                self.ma[i][5][str(t)] = n\n",
        "\n",
        "        self.upi_encoding = {}\n",
        "        hex_chars = list(range(10)) + ['A', 'B', 'C', 'D', 'E', 'F']\n",
        "        for n, char in enumerate(hex_chars):\n",
        "            self.upi_encoding[str(char)] = n\n",
        "\n",
        "    def _extract_uniprot_id(self, header: str) -> str:\n",
        "        \"\"\"Extract UniProt ID from header.\"\"\"\n",
        "        pos = header.find(\"UniRef\")\n",
        "        if pos == -1:\n",
        "            return \"\"\n",
        "\n",
        "        start = header.find('_', pos)\n",
        "        if start == -1:\n",
        "            return \"\"\n",
        "        start += 1\n",
        "\n",
        "        end = start\n",
        "        while end < len(header) and header[end] not in ' _\\t':\n",
        "            end += 1\n",
        "\n",
        "        uid = header[start:end]\n",
        "\n",
        "        # Validate - including UPI IDs\n",
        "        if len(uid) >= 3 and uid[:3] == \"UPI\":\n",
        "            return uid\n",
        "\n",
        "        # Regular UniProt ID validation\n",
        "        if len(uid) not in [6, 10]:\n",
        "            return \"\"\n",
        "        if not uid[0].isalpha():\n",
        "            return \"\"\n",
        "\n",
        "        return uid\n",
        "\n",
        "    def _uniprot_to_number(self, uniprot_ids: List[str]) -> List[int]:\n",
        "        \"\"\"Convert UniProt IDs to numbers for distance calculation.\"\"\"\n",
        "        numbers = []\n",
        "        for uni in uniprot_ids:\n",
        "            if not uni or not uni[0].isalpha():\n",
        "                numbers.append(0)\n",
        "                continue\n",
        "\n",
        "            # Handle UPI IDs\n",
        "            if uni.startswith(\"UPI\") and len(uni) == 13:\n",
        "                hex_part = uni[3:]  # Remove \"UPI\" prefix\n",
        "                num = 0\n",
        "                tot = 1\n",
        "\n",
        "                # Process hexadecimal characters in reverse order\n",
        "                for u in reversed(hex_part):\n",
        "                    if str(u) in self.upi_encoding:\n",
        "                        num += self.upi_encoding[str(u)] * tot\n",
        "                        tot *= 16  # Base 16 for hexadecimal\n",
        "                    else:\n",
        "                        # Invalid hex character, assign 0\n",
        "                        num = 0\n",
        "                        break\n",
        "                # Add offset to distinguish UPI IDs from standard ones\n",
        "                # Use a large offset to avoid collisions\n",
        "                numbers.append(num + 10**15)\n",
        "                continue\n",
        "\n",
        "            p = self.pa.get(uni[0], 0)\n",
        "            tot, num = 1, 0\n",
        "\n",
        "            if len(uni) == 10:\n",
        "                for n, u in enumerate(reversed(uni[-4:])):\n",
        "                    if str(u) in self.ma[p][n]:\n",
        "                        num += self.ma[p][n][str(u)] * tot\n",
        "                        tot *= len(self.ma[p][n].keys())\n",
        "\n",
        "            for n, u in enumerate(reversed(uni[:6])):\n",
        "                if n < len(self.ma[p]) and str(u) in self.ma[p][n]:\n",
        "                    num += self.ma[p][n][str(u)] * tot\n",
        "                    tot *= len(self.ma[p][n].keys())\n",
        "\n",
        "            numbers.append(num)\n",
        "\n",
        "        return numbers\n",
        "\n",
        "    def _calculate_genomic_distances(self, entry: Dict) -> List[int]:\n",
        "        \"\"\"Calculate sequential distances between adjacent chains.\"\"\"\n",
        "        distances = []\n",
        "        nums = entry['uniprot_nums']\n",
        "\n",
        "        for i in range(1, len(nums)):\n",
        "            if nums[i-1] and nums[i]:  # Both must be valid numbers\n",
        "                dist = abs(nums[i] - nums[i-1])\n",
        "                distances.append(dist)\n",
        "            else:\n",
        "                distances.append(-1)  # Invalid distance\n",
        "\n",
        "        return distances\n",
        "\n",
        "    def _get_cache_key(self, sequences: List[str], genomic_distance: Optional[int],\n",
        "                       prefix: Optional[str]) -> str:\n",
        "        \"\"\"Generate a unique cache key for the request\"\"\"\n",
        "        # Always use genomic_distance=20 for caching to maximize reuse\n",
        "        cache_genomic_distance = 20 if genomic_distance is not None else None\n",
        "\n",
        "        # Create a deterministic string representation\n",
        "        cache_data = {\n",
        "            'sequences': sequences,\n",
        "            'genomic_distance': cache_genomic_distance,\n",
        "            'prefix': prefix,\n",
        "            'host_url': self.host_url\n",
        "        }\n",
        "        cache_str = json.dumps(cache_data, sort_keys=True)\n",
        "\n",
        "        # Generate hash\n",
        "        cache_hash = hashlib.sha256(cache_str.encode()).hexdigest()[:16]\n",
        "\n",
        "        # Create human-readable prefix\n",
        "        seq_info = f\"{len(sequences)}seq\"\n",
        "        if prefix:\n",
        "            seq_info += f\"_{prefix}\"\n",
        "\n",
        "        return f\"{seq_info}_{cache_hash}\"\n",
        "\n",
        "    def _load_from_cache(self, cache_key: str) -> bool:\n",
        "        \"\"\"Try to load parsed entries from cache\"\"\"\n",
        "        cache_file = self.cache_dir / f\"{cache_key}.pkl\"\n",
        "\n",
        "        if cache_file.exists():\n",
        "            try:\n",
        "                with open(cache_file, 'rb') as f:\n",
        "                    cache_data = pickle.load(f)\n",
        "\n",
        "                self.parsed_entries = cache_data['parsed_entries']\n",
        "                self.job_id = cache_data.get('job_id', f\"cached_{cache_key}\")\n",
        "\n",
        "                print(f\"Loaded from cache: {cache_key}\")\n",
        "                return True\n",
        "            except Exception as e:\n",
        "                print(f\"Cache load failed: {e}\")\n",
        "                return False\n",
        "\n",
        "        return False\n",
        "\n",
        "    def _save_to_cache(self, cache_key: str):\n",
        "        \"\"\"Save parsed entries to cache\"\"\"\n",
        "        cache_file = self.cache_dir / f\"{cache_key}.pkl\"\n",
        "\n",
        "        try:\n",
        "            cache_data = {\n",
        "                'parsed_entries': self.parsed_entries,\n",
        "                'job_id': self.job_id,\n",
        "                'timestamp': time.time()\n",
        "            }\n",
        "\n",
        "            with open(cache_file, 'wb') as f:\n",
        "                pickle.dump(cache_data, f)\n",
        "\n",
        "            print(f\"Saved to cache: {cache_key}\")\n",
        "\n",
        "            # Also save a human-readable info file\n",
        "            info_file = self.cache_dir / f\"{cache_key}_info.json\"\n",
        "            info_data = {\n",
        "                'job_id': self.job_id,\n",
        "                'num_entries': len(self.parsed_entries),\n",
        "                'num_chains': len(self.parsed_entries[0]['sequences']) if self.parsed_entries else 0,\n",
        "                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(cache_data['timestamp']))\n",
        "            }\n",
        "            with open(info_file, 'w') as f:\n",
        "                json.dump(info_data, f, indent=2)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Cache save failed: {e}\")\n",
        "\n",
        "    def clear_cache(self, older_than_days: Optional[int] = None):\n",
        "        \"\"\"Clear cache files, optionally only those older than specified days\"\"\"\n",
        "        import glob\n",
        "\n",
        "        cache_files = glob.glob(str(self.cache_dir / \"*.pkl\"))\n",
        "        removed = 0\n",
        "\n",
        "        for cache_file in cache_files:\n",
        "            if older_than_days is not None:\n",
        "                # Check age\n",
        "                age_days = (time.time() - os.path.getmtime(cache_file)) / (24 * 3600)\n",
        "                if age_days < older_than_days:\n",
        "                    continue\n",
        "\n",
        "            try:\n",
        "                os.remove(cache_file)\n",
        "                # Also remove info file if exists\n",
        "                info_file = cache_file.replace('.pkl', '_info.json')\n",
        "                if os.path.exists(info_file):\n",
        "                    os.remove(info_file)\n",
        "                removed += 1\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        print(f\"Removed {removed} cache files\")\n",
        "\n",
        "    def submit_or_load_from_cache(self,\n",
        "                                  sequences: List[str],\n",
        "                                  genomic_distance: Optional[int] = 20,\n",
        "                                  prefix: Optional[str] = None,\n",
        "                                  use_cache: bool = True) -> Tuple[str, bool]:\n",
        "        \"\"\"Submit sequences or load from cache if available\n",
        "\n",
        "        Always uses genomic_distance=20 for caching to maximize reuse.\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (job_id, from_cache) where from_cache indicates if data was loaded from cache\n",
        "        \"\"\"\n",
        "        # Always use distance=20 for caching\n",
        "        cache_genomic_distance = 20 if genomic_distance is not None else None\n",
        "\n",
        "        # Generate cache key\n",
        "        cache_key = self._get_cache_key(sequences, cache_genomic_distance, prefix)\n",
        "        self._current_cache_key = cache_key\n",
        "\n",
        "        # Try to load from cache\n",
        "        if use_cache and self._load_from_cache(cache_key):\n",
        "            return self.job_id, True\n",
        "\n",
        "        # If not in cache, submit normally with distance=20\n",
        "        self.submit(sequences, cache_genomic_distance, prefix)\n",
        "        return self.job_id, False\n",
        "\n",
        "    def submit(self,\n",
        "               sequences: List[str],\n",
        "               genomic_distance: Optional[int] = 20,\n",
        "               prefix: Optional[str] = None) -> str:\n",
        "        \"\"\"Submit sequences and return job ID\"\"\"\n",
        "        # Create query\n",
        "        query = \"\"\n",
        "        for i, seq in enumerate(sequences, start=101):\n",
        "            if prefix:\n",
        "                query += f\">{prefix}_{i}\\n{seq}\\n\"\n",
        "            else:\n",
        "                query += f\">{i}\\n{seq}\\n\"\n",
        "\n",
        "        # Determine mode based on number of sequences\n",
        "        if len(sequences) == 1:\n",
        "            # Single sequence - use regular MSA mode\n",
        "            mode = \"env\"\n",
        "            endpoint = \"ticket/msa\"\n",
        "        else:\n",
        "            # Multiple sequences - use pairing mode\n",
        "            if genomic_distance is None:\n",
        "                mode = \"paircomplete\"\n",
        "            else:\n",
        "                mode = f\"paircomplete-pairfilterprox_{genomic_distance}\"\n",
        "            endpoint = \"ticket/pair\"\n",
        "\n",
        "        response = requests.post(\n",
        "            f'{self.host_url}/{endpoint}',\n",
        "            data={'q': query, 'mode': mode},\n",
        "            timeout=30\n",
        "        )\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            raise Exception(f\"Failed to submit: {response.text}\")\n",
        "\n",
        "        self.job_id = response.json()['id']\n",
        "        print(f\"Job submitted: {self.job_id} with mode: {mode}\")\n",
        "        return self.job_id\n",
        "\n",
        "    def wait(self, check_interval: int = 5):\n",
        "        \"\"\"Wait for job completion\"\"\"\n",
        "        while True:\n",
        "            response = requests.get(f'{self.host_url}/ticket/{self.job_id}', timeout=30)\n",
        "            status = response.json().get('status', 'UNKNOWN')\n",
        "            print(f\"Status: {status}\")\n",
        "\n",
        "            if status == \"COMPLETE\":\n",
        "                break\n",
        "            elif status == \"ERROR\":\n",
        "                raise Exception(\"Job failed\")\n",
        "            time.sleep(check_interval)\n",
        "\n",
        "    def download_and_parse(self, output_dir: str = \"results\"):\n",
        "        \"\"\"Download results and parse the MSA\"\"\"\n",
        "        # Download\n",
        "        Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "        tar_path = os.path.join(output_dir, f\"{self.job_id}.tar.gz\")\n",
        "\n",
        "        response = requests.get(f'{self.host_url}/result/download/{self.job_id}', timeout=60)\n",
        "        with open(tar_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "        # Extract\n",
        "        with tarfile.open(tar_path) as tar:\n",
        "            tar.extractall(output_dir)\n",
        "\n",
        "        # Check if this is a paired MSA or single sequence MSA\n",
        "        pair_a3m = os.path.join(output_dir, 'pair.a3m')\n",
        "        if os.path.exists(pair_a3m):\n",
        "            # Paired MSA\n",
        "            self.parsed_entries = self._parse_paired_a3m(pair_a3m)\n",
        "        else:\n",
        "            # Single sequence - combine multiple MSA files\n",
        "            self.parsed_entries = self._parse_single_msas(output_dir)\n",
        "\n",
        "        # Save to cache if we have a cache key\n",
        "        if hasattr(self, '_current_cache_key'):\n",
        "            self._save_to_cache(self._current_cache_key)\n",
        "\n",
        "    def _parse_msa_lines(self, lines: List[str]) -> List[Dict]:\n",
        "        \"\"\"Parse MSA lines into structured entries with UniProt ID extraction\"\"\"\n",
        "        entries = []\n",
        "        i = 0\n",
        "        is_first = True\n",
        "\n",
        "        while i < len(lines):\n",
        "            line = lines[i].rstrip()\n",
        "\n",
        "            if line.startswith('>'):\n",
        "                header = line\n",
        "                seq_lines = []\n",
        "                i += 1\n",
        "\n",
        "                # Collect sequence lines\n",
        "                while i < len(lines) and not lines[i].startswith('>'):\n",
        "                    if lines[i].strip():\n",
        "                        seq_lines.append(lines[i].rstrip())\n",
        "                    i += 1\n",
        "\n",
        "                sequence = ''.join(seq_lines)\n",
        "\n",
        "                # Parse header\n",
        "                header_parts = header.split('\\t')\n",
        "                header_clean = header_parts[0].lstrip('>').replace('UniRef100_', '')\n",
        "\n",
        "                # Extract UniProt ID\n",
        "                uid = self._extract_uniprot_id(header)\n",
        "                has_uniref = \"UniRef\" in header\n",
        "                uniprot_num = 0\n",
        "\n",
        "                if uid:\n",
        "                    # Convert to number\n",
        "                    uniprot_nums = self._uniprot_to_number([uid])\n",
        "                    uniprot_num = uniprot_nums[0] if uniprot_nums else 0\n",
        "\n",
        "                # For query sequence\n",
        "                if is_first:\n",
        "                    coverage = 1.0\n",
        "                    identity = 1.0\n",
        "                    evalue = 0.0\n",
        "                    alnscore = float('inf')\n",
        "                    is_first = False\n",
        "                else:\n",
        "                    # Extract metadata from header\n",
        "                    coverage = None\n",
        "                    identity = None\n",
        "                    evalue = None\n",
        "                    alnscore = None\n",
        "\n",
        "                    if len(header_parts) >= 10:\n",
        "                        try:\n",
        "                            alnscore = float(header_parts[1])\n",
        "                            identity = float(header_parts[2])\n",
        "                            evalue = float(header_parts[3])\n",
        "                            q_start = int(header_parts[4])\n",
        "                            q_end = int(header_parts[5])\n",
        "                            q_len = int(header_parts[6])\n",
        "                            coverage = (q_end - q_start + 1) / q_len\n",
        "                        except:\n",
        "                            pass\n",
        "\n",
        "                    # Fallback values\n",
        "                    if coverage is None:\n",
        "                        coverage = 0.0\n",
        "                    if identity is None:\n",
        "                        identity = 0.0\n",
        "                    if evalue is None:\n",
        "                        evalue = float('inf')\n",
        "                    if alnscore is None:\n",
        "                        alnscore = 0.0\n",
        "\n",
        "                entries.append({\n",
        "                    'header': header_clean,\n",
        "                    'sequence': sequence,\n",
        "                    'coverage': coverage,\n",
        "                    'identity': identity,\n",
        "                    'evalue': evalue,\n",
        "                    'alnscore': alnscore,\n",
        "                    'uid': uid,\n",
        "                    'uniprot_num': uniprot_num,\n",
        "                    'has_uniref': has_uniref\n",
        "                })\n",
        "            else:\n",
        "                i += 1\n",
        "\n",
        "        return entries\n",
        "\n",
        "    def _parse_paired_a3m(self, a3m_path: str) -> List[Dict]:\n",
        "        \"\"\"Parse paired A3M file into list of entries with UniProt ID tracking\"\"\"\n",
        "        # First, separate MSAs by chain ID\n",
        "        raw_msas = {}\n",
        "        update_M = True\n",
        "        M = None\n",
        "\n",
        "        with open(a3m_path, 'r') as f:\n",
        "            for line in f:\n",
        "                if \"\\x00\" in line:\n",
        "                    line = line.replace(\"\\x00\", \"\")\n",
        "                    update_M = True\n",
        "\n",
        "                if line.startswith(\">\") and update_M:\n",
        "                    # Extract chain ID (101, 102, etc.)\n",
        "                    M = int(line[1:].rstrip().split('_')[-1])\n",
        "                    update_M = False\n",
        "                    if M not in raw_msas:\n",
        "                        raw_msas[M] = []\n",
        "\n",
        "                if M is not None:\n",
        "                    raw_msas[M].append(line.rstrip())\n",
        "\n",
        "        # Parse each chain's MSA\n",
        "        parsed_msas = {}\n",
        "        for seq_id, lines in raw_msas.items():\n",
        "            parsed_msas[seq_id] = self._parse_msa_lines(lines)\n",
        "\n",
        "        # Get sorted chain IDs\n",
        "        seq_ids = sorted(parsed_msas.keys())\n",
        "\n",
        "        # IMPORTANT: All chains should have the same number of sequences when paired\n",
        "        num_entries_per_chain = [len(parsed_msas[sid]) for sid in seq_ids]\n",
        "        if len(set(num_entries_per_chain)) > 1:\n",
        "            print(f\"Warning: Chains have different numbers of sequences: {dict(zip(seq_ids, num_entries_per_chain))}\")\n",
        "            print(\"Taking minimum to preserve pairing\")\n",
        "\n",
        "        min_entries = min(num_entries_per_chain)\n",
        "\n",
        "        # Stitch entries together - sequences at the same position are paired\n",
        "        stitched_entries = []\n",
        "        for i in range(min_entries):\n",
        "            # Collect info from each chain at position i\n",
        "            headers = []\n",
        "            sequences = []\n",
        "            coverages = []\n",
        "            identities = []\n",
        "            evalues = []\n",
        "            alnscores = []\n",
        "            uids = []\n",
        "            uniprot_nums = []\n",
        "            has_uniref = True  # Will be False if any chain doesn't have UniRef\n",
        "\n",
        "            for sid in seq_ids:\n",
        "                entry = parsed_msas[sid][i]\n",
        "                headers.append(entry['header'])\n",
        "                sequences.append(entry['sequence'])\n",
        "                coverages.append(entry['coverage'])\n",
        "                identities.append(entry['identity'])\n",
        "                evalues.append(entry['evalue'])\n",
        "                alnscores.append(entry['alnscore'])\n",
        "                uids.append(entry['uid'])\n",
        "                uniprot_nums.append(entry['uniprot_num'])\n",
        "                has_uniref = has_uniref and entry['has_uniref']\n",
        "\n",
        "            stitched_entries.append({\n",
        "                'headers': headers,\n",
        "                'sequences': sequences,\n",
        "                'coverages': coverages,\n",
        "                'identities': identities,\n",
        "                'evalues': evalues,\n",
        "                'alnscores': alnscores,\n",
        "                'uids': uids,\n",
        "                'uniprot_nums': uniprot_nums,\n",
        "                'has_uniref': has_uniref,\n",
        "                'is_query': (i == 0)\n",
        "            })\n",
        "\n",
        "        return stitched_entries\n",
        "\n",
        "    def _parse_single_msas(self, output_dir: str) -> List[Dict]:\n",
        "        \"\"\"Parse and combine single sequence MSAs\"\"\"\n",
        "        # MSA files to look for\n",
        "        msa_files = ['uniref.a3m', 'bfd.mgnify30.metaeuk30.smag30.a3m']\n",
        "\n",
        "        all_entries = []\n",
        "        seen_sequences = set()\n",
        "\n",
        "        for msa_file in msa_files:\n",
        "            msa_path = os.path.join(output_dir, msa_file)\n",
        "            if os.path.exists(msa_path):\n",
        "                with open(msa_path, 'r') as f:\n",
        "                    lines = f.readlines()\n",
        "\n",
        "                entries = self._parse_msa_lines(lines)\n",
        "\n",
        "                # Add non-duplicate entries\n",
        "                for entry in entries:\n",
        "                    if entry['sequence'] not in seen_sequences:\n",
        "                        seen_sequences.add(entry['sequence'])\n",
        "                        all_entries.append({\n",
        "                            'headers': [entry['header']],\n",
        "                            'sequences': [entry['sequence']],\n",
        "                            'coverages': [entry['coverage']],\n",
        "                            'identities': [entry['identity']],\n",
        "                            'evalues': [entry['evalue']],\n",
        "                            'alnscores': [entry['alnscore']],\n",
        "                            'uids': [entry['uid']],\n",
        "                            'uniprot_nums': [entry['uniprot_num']],\n",
        "                            'has_uniref': entry['has_uniref'],\n",
        "                            'is_query': len(all_entries) == 0\n",
        "                        })\n",
        "\n",
        "        return all_entries\n",
        "\n",
        "    def save_msa(self,\n",
        "                 output_file: str,\n",
        "                 min_coverage: Optional[float] = None,\n",
        "                 min_identity: Optional[float] = None,\n",
        "                 max_evalue: Optional[float] = None,\n",
        "                 min_alnscore: Optional[float] = None,\n",
        "                 max_genomic_distance: Optional[int] = None) -> Tuple[int, List[Dict]]:\n",
        "        \"\"\"Save MSA with optional filtering including genomic distance\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (number of sequences written, list of filtered entries)\n",
        "        \"\"\"\n",
        "        if not self.parsed_entries:\n",
        "            raise ValueError(\"No MSA loaded. Run download_and_parse first.\")\n",
        "\n",
        "        # Smart parsing: convert percentages to fractions\n",
        "        if min_coverage is not None and min_coverage > 1:\n",
        "            min_coverage = min_coverage / 100\n",
        "        if min_identity is not None and min_identity > 1:\n",
        "            min_identity = min_identity / 100\n",
        "\n",
        "        sequences_written = 0\n",
        "        sequences_filtered = 0\n",
        "        filtered_entries = []  # Store entries that pass the filter\n",
        "\n",
        "        num_chains = len(self.parsed_entries[0]['sequences']) if self.parsed_entries else 0\n",
        "\n",
        "        with open(output_file, 'w') as f:\n",
        "            for entry in self.parsed_entries:\n",
        "                # Skip filtering for query sequence\n",
        "                if not entry['is_query']:\n",
        "                    # All chains must pass the filter\n",
        "                    filter_reasons = []\n",
        "\n",
        "                    if min_coverage and any(c < min_coverage for c in entry['coverages']):\n",
        "                        filter_reasons.append(f\"coverage < {min_coverage}\")\n",
        "                    if min_identity and any(i < min_identity for i in entry['identities']):\n",
        "                        filter_reasons.append(f\"identity < {min_identity}\")\n",
        "                    if max_evalue is not None and any(e > max_evalue for e in entry['evalues'] if e is not None):\n",
        "                        filter_reasons.append(f\"evalue > {max_evalue}\")\n",
        "                    if min_alnscore is not None and any(a < min_alnscore for a in entry['alnscores'] if a is not None):\n",
        "                        filter_reasons.append(f\"alnscore < {min_alnscore}\")\n",
        "\n",
        "                    # Genomic distance filtering\n",
        "                    if max_genomic_distance is not None and entry['has_uniref']:\n",
        "                        distances = self._calculate_genomic_distances(entry)\n",
        "\n",
        "                        if num_chains == 2:\n",
        "                            # Simple case: check single distance\n",
        "                            if distances[0] != -1 and distances[0] > max_genomic_distance:\n",
        "                                filter_reasons.append(f\"genomic distance > {max_genomic_distance}\")\n",
        "                        else:\n",
        "                            # For >2 chains: check if all distances exceed threshold\n",
        "                            # (relaxed filtering - keep if ANY distance is within threshold)\n",
        "                            valid_distances = [d for d in distances if d != -1]\n",
        "                            if valid_distances and all(d > max_genomic_distance for d in valid_distances):\n",
        "                                filter_reasons.append(f\"all genomic distances > {max_genomic_distance}\")\n",
        "\n",
        "                    if filter_reasons:\n",
        "                        sequences_filtered += 1\n",
        "                        continue\n",
        "\n",
        "                # Write entry with modified header\n",
        "                if entry['is_query']:\n",
        "                    # Query header format: query_len1_len2_len3\n",
        "                    header = \"query\"\n",
        "                    for seq in entry['sequences']:\n",
        "                        header += f\"_len{len(seq)}\"\n",
        "                else:\n",
        "                    # Regular header format: UID1_UID2_UID3_dist1-2_dist2-3\n",
        "                    # First write UIDs (or original headers if no UID)\n",
        "                    header_parts = []\n",
        "                    for i, uid in enumerate(entry['uids']):\n",
        "                        if uid:\n",
        "                            header_parts.append(uid)\n",
        "                        else:\n",
        "                            header_parts.append(entry['headers'][i])\n",
        "\n",
        "                    header = '_'.join(header_parts)\n",
        "\n",
        "                    # Add distances if we have valid UIDs\n",
        "                    if entry['has_uniref'] and all(entry['uids']):\n",
        "                        distances = self._calculate_genomic_distances(entry)\n",
        "                        for dist in distances:\n",
        "                            if dist != -1:\n",
        "                                header += f\"_{dist}\"\n",
        "\n",
        "                sequence = ''.join(entry['sequences']).replace('\\x00','')\n",
        "                f.write(f\">{header}\\n{sequence}\\n\")\n",
        "                sequences_written += 1\n",
        "                filtered_entries.append(entry)\n",
        "\n",
        "        print(f\"Saved {sequences_written} sequences to {output_file}\")\n",
        "        if sequences_filtered > 0:\n",
        "            print(f\"Filtered out {sequences_filtered} sequences\")\n",
        "        return sequences_written, filtered_entries\n",
        "\n",
        "    def get_stats(self, entries: Optional[List[Dict]] = None) -> Dict:\n",
        "        \"\"\"Get statistics about the MSA\n",
        "\n",
        "        Args:\n",
        "            entries: Optional list of entries to calculate stats from.\n",
        "                    If None, uses all parsed entries.\n",
        "        \"\"\"\n",
        "        if entries is None:\n",
        "            entries = self.parsed_entries\n",
        "\n",
        "        if not entries:\n",
        "            return {}\n",
        "\n",
        "        num_chains = len(entries[0]['sequences']) if entries else 0\n",
        "\n",
        "        stats = {\n",
        "            'num_chains': num_chains,\n",
        "            'num_entries': len(entries),\n",
        "        }\n",
        "\n",
        "        # Per-chain statistics\n",
        "        for i in range(num_chains):\n",
        "            coverages = [e['coverages'][i] for e in entries[1:]]  # Skip query\n",
        "            identities = [e['identities'][i] for e in entries[1:]]\n",
        "            evalues = [e['evalues'][i] for e in entries[1:] if e['evalues'][i] is not None]\n",
        "            alnscores = [e['alnscores'][i] for e in entries[1:] if e['alnscores'][i] is not None]\n",
        "\n",
        "            chain_id = i + 101\n",
        "            stats[f'chain_{chain_id}'] = {\n",
        "                'query_length': len(entries[0]['sequences'][i]) if entries else 0,\n",
        "                'avg_coverage': sum(coverages) / len(coverages) if coverages else 0,\n",
        "                'avg_identity': sum(identities) / len(identities) if identities else 0,\n",
        "                'avg_evalue': sum(evalues) / len(evalues) if evalues else 0,\n",
        "                'avg_alnscore': sum(alnscores) / len(alnscores) if alnscores else 0,\n",
        "                'min_evalue': min(evalues) if evalues else None,\n",
        "                'max_alnscore': max(alnscores) if alnscores else None,\n",
        "            }\n",
        "\n",
        "        return stats\n",
        "\n",
        "def get_paired_msa(sequences: Union[str, List[str]],\n",
        "                   output_file: str,\n",
        "                   genomic_distance: Optional[int] = 20,\n",
        "                   min_coverage: Optional[float] = None,\n",
        "                   min_identity: Optional[float] = None,\n",
        "                   max_evalue: Optional[float] = None,\n",
        "                   min_alnscore: Optional[float] = None,\n",
        "                   prefix: Optional[str] = None,\n",
        "                   host_url: str = \"https://api.colabfold.com\",\n",
        "                   cache_dir: Optional[str] = None,\n",
        "                   use_cache: bool = True,\n",
        "                   keep_temp: bool = False) -> str:\n",
        "    \"\"\"\n",
        "    Simple wrapper to get paired MSA from ColabFold with extended filtering\n",
        "\n",
        "    Args:\n",
        "        sequences: List of protein sequences or a single string with ':' delimiter\n",
        "        output_file: Path to save the stitched MSA\n",
        "        genomic_distance: Genomic distance for pairing (default: 20, None for no filtering)\n",
        "        min_coverage: Minimum coverage filter (0-1 or 0-100 for percentage)\n",
        "        min_identity: Minimum identity filter (0-1 or 0-100 for percentage)\n",
        "        max_evalue: Maximum e-value filter (e.g., 1e-5, 0.001)\n",
        "        min_alnscore: Minimum alignment score filter\n",
        "        prefix: Optional prefix for sequence IDs\n",
        "        host_url: ColabFold API URL\n",
        "        cache_dir: Directory for caching results (default: ~/.colabfold_cache)\n",
        "        use_cache: Whether to use caching (default: True)\n",
        "        keep_temp: If True, keep temporary files for debugging (default: False)\n",
        "\n",
        "    Returns:\n",
        "        Path to the output file\n",
        "    \"\"\"\n",
        "    # Handle string input\n",
        "    if isinstance(sequences, str):\n",
        "        # Split by ':' and clean up\n",
        "        sequences = [seq.strip() for seq in sequences.split(':') if seq.strip()]\n",
        "\n",
        "    # Further cleanup of sequences\n",
        "    cleaned_sequences = []\n",
        "    for seq in sequences:\n",
        "        # Remove whitespace and convert to uppercase\n",
        "        seq = ''.join(seq.split()).upper()\n",
        "        # Only add non-empty sequences\n",
        "        if seq:\n",
        "            cleaned_sequences.append(seq)\n",
        "\n",
        "    if not cleaned_sequences:\n",
        "        raise ValueError(\"No valid sequences provided\")\n",
        "\n",
        "    sequences = cleaned_sequences\n",
        "\n",
        "    # Store user's requested genomic distance\n",
        "    user_genomic_distance = genomic_distance\n",
        "\n",
        "    # Always fetch with distance=20 for better caching\n",
        "    fetch_genomic_distance = 20 if genomic_distance is not None else None\n",
        "\n",
        "    # Create temp directory\n",
        "    if keep_temp:\n",
        "        # Create a permanent temp directory\n",
        "        temp_dir = tempfile.mkdtemp(prefix=\"colabfold_\")\n",
        "        print(f\"Temporary files will be kept in: {temp_dir}\")\n",
        "    else:\n",
        "        # Use context manager for automatic cleanup\n",
        "        temp_context = tempfile.TemporaryDirectory()\n",
        "        temp_dir = temp_context.__enter__()\n",
        "\n",
        "    try:\n",
        "        # Create handler with cache support\n",
        "        msa = ColabFoldPairedMSA(host_url, cache_dir)\n",
        "\n",
        "        # Store cache key for later use - always use distance=20 for caching\n",
        "        cache_key = msa._get_cache_key(sequences, fetch_genomic_distance, prefix)\n",
        "        msa._current_cache_key = cache_key\n",
        "\n",
        "        # Submit or load from cache\n",
        "        job_id, from_cache = msa.submit_or_load_from_cache(sequences, fetch_genomic_distance, prefix, use_cache)\n",
        "\n",
        "        # Only wait and download if it's a new job (not from cache)\n",
        "        if not from_cache:\n",
        "            msa.wait()\n",
        "            # Download and parse\n",
        "            msa.download_and_parse(temp_dir)\n",
        "\n",
        "        # Save MSA to temporary file first with user's requested distance filtering\n",
        "        temp_output = os.path.join(temp_dir, \"temp_output.a3m\")\n",
        "        num_sequences, filtered_entries = msa.save_msa(\n",
        "            temp_output,\n",
        "            min_coverage,\n",
        "            min_identity,\n",
        "            max_evalue,\n",
        "            min_alnscore,\n",
        "            max_genomic_distance=user_genomic_distance\n",
        "        )\n",
        "\n",
        "        # Move to final location\n",
        "        shutil.move(temp_output, output_file)\n",
        "\n",
        "        print(f\"\\nMSA saved to: {output_file}\")\n",
        "\n",
        "        # Print stats BEFORE filtering\n",
        "        stats_before = msa.get_stats()\n",
        "        print(f\"\\n=== Statistics BEFORE filtering ===\")\n",
        "        print(f\"Total entries: {stats_before['num_entries']}\")\n",
        "        for i in range(stats_before['num_chains']):\n",
        "            chain_stats = stats_before[f'chain_{i + 101}']\n",
        "            print(f\"Chain {i + 101}: query_length={chain_stats['query_length']}, \"\n",
        "                  f\"avg_coverage={chain_stats['avg_coverage']:.2f}, \"\n",
        "                  f\"avg_identity={chain_stats['avg_identity']:.2f}\"\n",
        "                  )\n",
        "\n",
        "        # Print stats AFTER filtering (if any filtering was applied)\n",
        "        if any([min_coverage, min_identity, max_evalue is not None, min_alnscore is not None, user_genomic_distance != fetch_genomic_distance]):\n",
        "            stats_after = msa.get_stats(filtered_entries)\n",
        "            print(f\"\\n=== Statistics AFTER filtering ===\")\n",
        "            print(f\"Total entries: {stats_after['num_entries']} (saved)\")\n",
        "            for i in range(stats_after['num_chains']):\n",
        "                chain_stats = stats_after[f'chain_{i + 101}']\n",
        "                print(f\"Chain {i + 101}: query_length={chain_stats['query_length']}, \"\n",
        "                      f\"avg_coverage={chain_stats['avg_coverage']:.2f}, \"\n",
        "                      f\"avg_identity={chain_stats['avg_identity']:.2f}\"\n",
        "                      )\n",
        "\n",
        "        if keep_temp:\n",
        "            print(f\"\\nTemporary files kept in: {temp_dir}\")\n",
        "            print(\"Files:\")\n",
        "            for file in os.listdir(temp_dir):\n",
        "                print(f\"  - {file}\")\n",
        "\n",
        "    finally:\n",
        "        # Clean up if not keeping temp files\n",
        "        if not keep_temp:\n",
        "            temp_context.__exit__(None, None, None)\n",
        "\n",
        "    return output_file\n",
        "\n",
        "\n",
        "def get_unique_jobname(base_jobname):\n",
        "    \"\"\"Get a unique jobname by incrementing if directory already exists.\"\"\"\n",
        "    if not os.path.exists(base_jobname):\n",
        "        return base_jobname\n",
        "\n",
        "    counter = 1\n",
        "    while os.path.exists(f\"{base_jobname}_{counter}\"):\n",
        "        counter += 1\n",
        "\n",
        "    return f\"{base_jobname}_{counter}\"\n",
        "\n",
        "def prepare_sequences(sequence, remove_duplicates = True):\n",
        "    \"\"\"\n",
        "    Clean and prepare sequences from input string.\n",
        "\n",
        "    Args:\n",
        "        sequence: Raw sequence string, chains separated by ':'\n",
        "        remove_duplicates: If True, removes duplicate sequences while preserving order\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (cleaned sequences, chain break indices)\n",
        "    \"\"\"\n",
        "    # Clean sequence\n",
        "    sequence = sequence.upper()\n",
        "    sequence = re.sub(\"[^A-Z:/()]\", \"\", sequence)\n",
        "    sequence = re.sub(r\"\\(\", \":(\", sequence)\n",
        "    sequence = re.sub(r\"\\)\", \"):\", sequence)\n",
        "    sequence = re.sub(\":+\", \":\", sequence)\n",
        "    sequence = re.sub(\"/+\", \"/\", sequence)\n",
        "    sequence = re.sub(\"^[:/]+\", \"\", sequence)\n",
        "    sequence = re.sub(\"[:/]+$\", \"\", sequence)\n",
        "\n",
        "    # Split into individual sequences\n",
        "    sequences = sequence.split(\":\")\n",
        "    sequences = [seq for seq in sequences if seq]\n",
        "\n",
        "    # Remove duplicates while preserving order\n",
        "    if remove_duplicates and len(sequences) > 1:\n",
        "        seen = set()\n",
        "        unique_sequences = []\n",
        "\n",
        "        for seq in sequences:\n",
        "            if seq not in seen:\n",
        "                seen.add(seq)\n",
        "                unique_sequences.append(seq)\n",
        "\n",
        "        if len(unique_sequences) < len(sequences):\n",
        "            print(f\"Note: Removed {len(sequences) - len(unique_sequences)} duplicate sequence(s)\")\n",
        "\n",
        "        sequences = unique_sequences\n",
        "\n",
        "    # Calculate chain break indices for the final sequences\n",
        "    chain_breaks = []\n",
        "    position = 0\n",
        "    for i, seq in enumerate(sequences[:-1]):  # All except last sequence\n",
        "        position += len(seq)\n",
        "        chain_breaks.append(position)\n",
        "\n",
        "    return sequences, chain_breaks\n",
        "\n",
        "#################################################################################################\n",
        "def convert_to_numpy(obj):\n",
        "    \"\"\"\n",
        "    Recursively convert PyTorch tensors to numpy arrays, handling BFloat16 and nested structures.\n",
        "    \"\"\"\n",
        "    if isinstance(obj, torch.Tensor):\n",
        "        # Convert BFloat16 to Float32 first, then to numpy\n",
        "        if obj.dtype == torch.bfloat16:\n",
        "            return obj.float().cpu().numpy()\n",
        "        else:\n",
        "            return obj.cpu().numpy()\n",
        "    elif isinstance(obj, dict):\n",
        "        # Recursively convert dictionary values\n",
        "        return {key: convert_to_numpy(value) for key, value in obj.items()}\n",
        "    elif isinstance(obj, list):\n",
        "        # Recursively convert list elements\n",
        "        return [convert_to_numpy(item) for item in obj]\n",
        "    elif isinstance(obj, tuple):\n",
        "        # Recursively convert tuple elements\n",
        "        return tuple(convert_to_numpy(item) for item in obj)\n",
        "    else:\n",
        "        # Return as-is for non-tensor types\n",
        "        return obj\n",
        "\n",
        "def clear_gpu_memory(keep_model=True):\n",
        "    \"\"\"\n",
        "    Clear GPU memory while optionally keeping the model.\n",
        "    \"\"\"\n",
        "    # Get all objects in memory\n",
        "    for obj in gc.get_objects():\n",
        "        try:\n",
        "            if isinstance(obj, torch.Tensor):\n",
        "                # Skip model parameters if we want to keep the model\n",
        "                if keep_model and hasattr(obj, '_base') and obj._base is not None:\n",
        "                    continue\n",
        "                del obj\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # Multiple rounds of garbage collection\n",
        "    for _ in range(3):\n",
        "        gc.collect()\n",
        "\n",
        "    # Clear CUDA cache\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "#################################################################################################\n",
        "\n",
        "def run_msa_analysis(\n",
        "    msa_file,\n",
        "    sequences,\n",
        "    breaks=None,\n",
        "    max_msa_depth=512,\n",
        "    mode=\"contacts\",  # \"contacts\", \"conservation\", or \"jacobian\"\n",
        "    mutation_subset=None,  # For jacobian mode: which mutations to test\n",
        "    device=None,\n",
        "    show_progress=True,\n",
        "    use_query_biasing=True,\n",
        "    fix_weights=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Unified function for all MSA Pairformer analyses.\n",
        "\n",
        "    Args:\n",
        "        msa_file: Path to MSA file\n",
        "        sequences: List of protein sequences or a single string with ':' delimiter\n",
        "        breaks: Chain break indices (optional - will be computed from sequences if not provided)\n",
        "        max_msa_depth: Maximum MSA depth\n",
        "        mode: Analysis mode:\n",
        "            - \"contacts\": Returns contact map from contact head\n",
        "            - \"conservation\": Returns (L, 20) matrix of p(aa) at each position\n",
        "            - \"jacobian\": Categorical Jacobian with flexible mutation subset\n",
        "        mutation_subset: For jacobian mode, which mutations to test:\n",
        "            - None: all 20 amino acids (default)\n",
        "            - ['F', 'D', 'V']: specific amino acids\n",
        "            - [13, 3, 19]: amino acid indices (0-19)\n",
        "        device: Device to run on (defaults to CUDA if available)\n",
        "        show_progress: Whether to show progress bar (for conservation/jacobian modes)\n",
        "\n",
        "    Returns:\n",
        "        - If mode=\"contacts\": contact matrix (L, L) or full results dict\n",
        "        - If mode=\"conservation\": (L, 20) matrix of amino acid probabilities\n",
        "        - If mode=\"jacobian\": (L, K, L, 20) where K = len(mutation_subset) or 20 if None\n",
        "    \"\"\"\n",
        "    # Handle sequence input\n",
        "    if isinstance(sequences, str):\n",
        "        sequences = [seq.strip() for seq in sequences.split(':') if seq.strip()]\n",
        "\n",
        "    # Compute total length and breaks if not provided\n",
        "    total_length = sum(len(seq) for seq in sequences)\n",
        "    if breaks is None and len(sequences) > 1:\n",
        "        breaks = []\n",
        "        position = 0\n",
        "        for seq in sequences[:-1]:\n",
        "            position += len(seq)\n",
        "            breaks.append(position)\n",
        "    elif breaks is None:\n",
        "        breaks = []\n",
        "\n",
        "    try:\n",
        "        # Set device\n",
        "        if device is None:\n",
        "            device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # Clear memory before starting\n",
        "        clear_gpu_memory(keep_model=True)\n",
        "\n",
        "        # Load and process MSA\n",
        "        np.random.seed(42)\n",
        "        msa_obj = MSA(\n",
        "            msa_file_path=msa_file,\n",
        "            max_seqs=max_msa_depth,\n",
        "            max_length=total_length,\n",
        "            max_tokens=1e12,\n",
        "            diverse_select_method=\"hhfilter\",\n",
        "            hhfilter_kwargs={\"binary\": \"hhfilter\"}\n",
        "        )\n",
        "\n",
        "        # Store MSA depth\n",
        "        msa_depth = msa_obj.n_diverse_seqs\n",
        "\n",
        "        # Prepare MSA tensors\n",
        "        msa_tokenized_t = msa_obj.diverse_tokenized_msa\n",
        "        msa_onehot_t = torch.nn.functional.one_hot(msa_tokenized_t, num_classes=len(aa2tok_d)).unsqueeze(0).float().to(device)\n",
        "\n",
        "        # Prepare masks\n",
        "        mask, msa_mask, full_mask, pairwise_mask = prepare_msa_masks(msa_obj.diverse_tokenized_msa.unsqueeze(0))\n",
        "        mask = mask.to(device)\n",
        "        msa_mask = msa_mask.to(device)\n",
        "        full_mask = full_mask.to(device)\n",
        "        pairwise_mask = pairwise_mask.to(device)\n",
        "\n",
        "        # Get sequence length\n",
        "        seq_length = msa_onehot_t.shape[2]\n",
        "\n",
        "        # Set model to eval mode\n",
        "        global_model.eval()\n",
        "\n",
        "        # Configure query-biasing\n",
        "        if use_query_biasing:\n",
        "            global_model.turn_on_query_biasing()\n",
        "        else:\n",
        "            global_model.turn_off_query_biasing()\n",
        "\n",
        "        # Initialize common model kwargs\n",
        "        model_kwargs = {\n",
        "            'mask': mask,\n",
        "            'msa_mask': msa_mask,\n",
        "            'full_mask': full_mask,\n",
        "            'pairwise_mask': pairwise_mask,\n",
        "            'complex_chain_break_indices': [breaks] if breaks else None\n",
        "        }\n",
        "\n",
        "        # Initialize results dictionary\n",
        "        results= {}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Step 1: Always run a forward pass for logits, contacts, and sequence weights\n",
        "            with torch.amp.autocast(dtype=torch.bfloat16, device_type=\"cuda\"):\n",
        "                contacts_res = global_model(\n",
        "                    msa=msa_onehot_t.to(torch.bfloat16),\n",
        "                    return_seq_weights=True,\n",
        "                    **model_kwargs\n",
        "                )\n",
        "            # Store all contact results\n",
        "            contacts_numpy = convert_to_numpy(contacts_res)\n",
        "            results.update({\n",
        "                \"predicted_cb_contacts\": contacts_numpy[\"predicted_cb_contacts\"][0],\n",
        "                \"predicted_confind_contacts\": contacts_numpy[\"predicted_confind_contacts\"][0],\n",
        "                \"seq_weights_list_d\": contacts_numpy[\"seq_weights_list_d\"],\n",
        "                \"total_length\": total_length,\n",
        "                \"max_msa_depth\": max_msa_depth,\n",
        "                \"msa_depth\": msa_depth,\n",
        "                \"weight_scale\": msa_onehot_t.shape[1]\n",
        "            })\n",
        "\n",
        "            # Step 2: Run conservation and/or categorical jacobian analyses if specified\n",
        "            if mode in [\"conservation\", \"jacobian\", \"all\"]:\n",
        "                # Turn off query biasing if using fixed sequence weights from initial forward pass\n",
        "                if use_query_biasing and fix_weights:\n",
        "                    global_model.turn_off_query_biasing()\n",
        "                    model_kwargs['seq_weights_dict'] = contacts_res['seq_weights_list_d']\n",
        "                # Set up forward function based on sequence weighting strategy\n",
        "                def f(msa_input, return_probs=False):\n",
        "                    with torch.amp.autocast(dtype=torch.bfloat16, device_type=\"cuda\"):\n",
        "                        res = global_model(\n",
        "                            msa=msa_input.to(torch.bfloat16),\n",
        "                            return_seq_weights=False,\n",
        "                            return_contacts=False,\n",
        "                            query_only=True,\n",
        "                            **model_kwargs\n",
        "                        )\n",
        "                    logits = res['logits'][0, 0, :seq_length, :20].float()\n",
        "                    if return_probs:\n",
        "                        return torch.nn.functional.softmax(logits, dim=-1).cpu().numpy()\n",
        "                    else:\n",
        "                      return logits.cpu().numpy()\n",
        "\n",
        "                # Conservation analysis\n",
        "                if mode in [\"conservation\", \"all\"]:\n",
        "                    # Initialize conservation matrix\n",
        "                    conservation = np.zeros((seq_length, 20))\n",
        "                    # Mask each position and compute predicted profiles\n",
        "                    for n in range(seq_length):\n",
        "                        if show_progress:\n",
        "                            print(f\"\\rConservation: {n+1}/{seq_length} ({100*(n+1)//seq_length}%)\", end=\"\", flush=True)\n",
        "                        # Mask position\n",
        "                        msa_h = msa_onehot_t.clone()\n",
        "                        msa_h[0, 0, n, :] = 0\n",
        "                        msa_h[0, 0, n, aa2tok_d[\"<mask>\"]] = 1\n",
        "                        # Compute amino acid probabilities\n",
        "                        probs = f(msa_h, return_probs=True)\n",
        "                        conservation[n] = probs[n]\n",
        "                    if show_progress:\n",
        "                        print()\n",
        "                    results['conservation'] = conservation\n",
        "\n",
        "                if mode in [\"jacobian\", \"all\"]:\n",
        "                    # Get baseline logits\n",
        "                    fx = f(msa_onehot_t, return_probs=False)\n",
        "                    wt_sequence = msa_tokenized_t[0].cpu().numpy()\n",
        "\n",
        "                    # Parse mutation subset\n",
        "                    if mutation_subset is None:\n",
        "                        mutation_indices = list(range(20))\n",
        "                    else:\n",
        "                        mutation_indices = []\n",
        "                        for mut in mutation_subset:\n",
        "                            if isinstance(mut, str):\n",
        "                                assert mut in aa2tok_d, f\"Invalid mutation: {mut}\"\n",
        "                                mutation_indices.append(aa2tok_d[mut.upper()])\n",
        "                            else:\n",
        "                                assert int(mut) <= 27, f\"Invalid mutation index: {mut}\"\n",
        "                                mutation_indices.append(int(mut))\n",
        "\n",
        "                    # Initialize jacobian tensor and iterate over positions and mutations\n",
        "                    fx_h = np.zeros((seq_length, len(mutation_indices), seq_length, 20))\n",
        "                    for n in range(seq_length):\n",
        "                        if show_progress:\n",
        "                            print(f\"\\rJacobian: {n+1}/{seq_length} ({100*(n+1)//seq_length}%)\", end=\"\", flush=True)\n",
        "                        wt_aa = wt_sequence[n]\n",
        "                        for idx, mutation_aa in enumerate(mutation_indices):\n",
        "                            if mutation_aa == wt_aa and mutation_subset is None:\n",
        "                                fx_h[n, idx] = fx.copy()\n",
        "                            else:\n",
        "                                msa_h = msa_onehot_t.clone()\n",
        "                                msa_h[0, 0, n, :] = 0\n",
        "                                msa_h[0, 0, n, mutation_aa] = 1\n",
        "                                fx_h[n, idx] = f(msa_h, return_probs=False)\n",
        "                    if show_progress:\n",
        "                        print()\n",
        "                    # Compute delta\n",
        "                    results['jacobian'] = fx - fx_h\n",
        "\n",
        "        # Clean up\n",
        "        del msa_onehot_t, mask, msa_mask, full_mask, pairwise_mask, msa_tokenized_t, msa_obj\n",
        "        clear_gpu_memory(keep_model=True)\n",
        "\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in MSA analysis: {e}\")\n",
        "        clear_gpu_memory(keep_model=True)\n",
        "        raise\n",
        "\n",
        "#################################################################################################\n",
        "\n",
        "# Also include the jac_to_con function from your code\n",
        "def jac_to_con(jac, center=True, diag=\"remove\", apc=True,\n",
        "               symm_before=True, symm_after=False):\n",
        "    \"\"\"Convert Jacobian to contact map\"\"\"\n",
        "    X = jac.copy()\n",
        "    Lx, Ax, Ly, Ay = X.shape\n",
        "\n",
        "    if symm_before:\n",
        "        X = X + X.transpose(2, 3, 0, 1)\n",
        "\n",
        "    if center:\n",
        "        for i in range(4):\n",
        "            if X.shape[i] > 1:\n",
        "                X -= X.mean(i, keepdims=True)\n",
        "\n",
        "    contacts = np.sqrt(np.square(X).sum((1, 3)))\n",
        "\n",
        "    if symm_after:\n",
        "        contacts = contacts + contacts.T\n",
        "\n",
        "    if diag == \"remove\":\n",
        "        np.fill_diagonal(contacts, 0)\n",
        "\n",
        "    if diag == \"normalize\":\n",
        "        contacts_diag = np.diag(contacts)\n",
        "        contacts = contacts / np.sqrt(contacts_diag[:, None] * contacts_diag[None, :])\n",
        "\n",
        "    if apc:\n",
        "        ap = contacts.sum(0, keepdims=True) * contacts.sum(1, keepdims=True) / contacts.sum()\n",
        "        contacts = contacts - ap\n",
        "\n",
        "    if diag == \"remove\":\n",
        "        np.fill_diagonal(contacts, 0)\n",
        "\n",
        "    return contacts\n",
        "\n",
        "#################################################################################################\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from bokeh.io import output_notebook\n",
        "from bokeh.plotting import figure, show\n",
        "from bokeh.transform import linear_cmap\n",
        "from bokeh.palettes import gray, viridis, RdBu\n",
        "\n",
        "output_notebook()\n",
        "\n",
        "class ContactAnalyzer:\n",
        "    def __init__(self, contacts, sequences, breaks, title):\n",
        "        self.sequences = sequences\n",
        "        self.breaks = breaks if breaks else []\n",
        "        self.contacts = contacts\n",
        "        self._prepare_data()\n",
        "        self.title = title\n",
        "\n",
        "    def _prepare_data(self):\n",
        "        \"\"\"Create position mapping with chain info\"\"\"\n",
        "        full_seq = ''.join(self.sequences)\n",
        "        chain_starts = [0] + self.breaks\n",
        "        chain_ends = self.breaks + [len(full_seq)]\n",
        "\n",
        "        # Map each position to chain info\n",
        "        self.pos_info = {}\n",
        "        self.chain_info = {}\n",
        "        for i, (start, end) in enumerate(zip(chain_starts, chain_ends)):\n",
        "            chain = chr(65 + i)  # A, B, C...\n",
        "            seq = self.sequences[i]\n",
        "            for j in range(start, end):\n",
        "                pos_in_chain = j - start + 1\n",
        "                abs_pos = j + 1\n",
        "                self.pos_info[abs_pos] = f\"{chain}:{pos_in_chain}{seq[j - start]}\"\n",
        "                self.chain_info[abs_pos] = chain\n",
        "\n",
        "    def get_table(self, min_score=None):\n",
        "        \"\"\"Get contact table for display\"\"\"\n",
        "        data = []\n",
        "        n = self.contacts.shape[0]\n",
        "\n",
        "        for i in range(n):\n",
        "            for j in range(i + 1, n):  # Upper triangle only\n",
        "                if min_score is None or self.contacts[i, j] >= min_score:\n",
        "                    chain_i = self.chain_info[i + 1]\n",
        "                    chain_j = self.chain_info[j + 1]\n",
        "                    interaction = 'intra' if chain_i == chain_j else 'inter'\n",
        "\n",
        "                    data.append({\n",
        "                        'Residue i': self.pos_info[i + 1],\n",
        "                        'Residue j': self.pos_info[j + 1],\n",
        "                        'Score': f\"{self.contacts[i, j]:.3f}\",\n",
        "                        'Chain i': chain_i,\n",
        "                        'Chain j': chain_j,\n",
        "                        'Type': interaction\n",
        "                    })\n",
        "\n",
        "        df = pd.DataFrame(data)\n",
        "        return df.sort_values('Score', ascending=False)\n",
        "\n",
        "    def get_plot_data(self, threshold=None):\n",
        "        \"\"\"Get filtered contact data for plotting\"\"\"\n",
        "        data = []\n",
        "        n = self.contacts.shape[0]\n",
        "\n",
        "        for i in range(n):\n",
        "            for j in range(n):\n",
        "                if threshold is None or self.contacts[i, j] >= threshold:\n",
        "                    chain_i = self.chain_info[i + 1]\n",
        "                    chain_j = self.chain_info[j + 1]\n",
        "\n",
        "                    data.append({\n",
        "                        'i': str(i + 1),\n",
        "                        'j': str(j + 1),\n",
        "                        'value': self.contacts[i, j],\n",
        "                        'label_i': self.pos_info[i + 1],\n",
        "                        'label_j': self.pos_info[j + 1],\n",
        "                        'type': 'intra' if chain_i == chain_j else 'inter'\n",
        "                    })\n",
        "\n",
        "        return pd.DataFrame(data)\n",
        "\n",
        "    def plot(self, threshold=None, size=800):\n",
        "        \"\"\"Create bokeh plot\"\"\"\n",
        "        from bokeh.plotting import figure, show\n",
        "        from bokeh.transform import linear_cmap\n",
        "        from bokeh.palettes import gray\n",
        "\n",
        "        df = self.get_plot_data(threshold)\n",
        "        n = self.contacts.shape[0]\n",
        "\n",
        "        p = figure(\n",
        "            width=size, height=size,\n",
        "            x_range=[str(i) for i in range(1, n + 1)],\n",
        "            y_range=[str(i) for i in range(1, n + 1)][::-1],\n",
        "            tools=\"hover,save\",\n",
        "            tooltips=[\n",
        "                (\"Residue i\", \"@label_i\"),\n",
        "                (\"Residue j\", \"@label_j\"),\n",
        "                (\"Score\", \"@value{0.000}\"),\n",
        "                (\"Type\", \"@type\")\n",
        "            ],\n",
        "            title=self.title\n",
        "        )\n",
        "        p.title.text_font_size = '16pt'\n",
        "\n",
        "        p.rect(x=\"i\", y=\"j\", width=1, height=1, source=df,\n",
        "               fill_color=linear_cmap('value', gray(256)[::-1],\n",
        "                                      low=df.value.min(),\n",
        "                                      high=df.value.max()\n",
        "                                      ),\n",
        "               line_color=None)\n",
        "\n",
        "        # Add chain breaks\n",
        "        for b in self.breaks:\n",
        "            p.line([str(b + 1)] * 2, ['1', str(n)], color='red', width=2)\n",
        "            p.line(['1', str(n)], [str(b + 1)] * 2, color='red', width=2)\n",
        "\n",
        "        p.xaxis.visible = False\n",
        "        p.yaxis.visible = False\n",
        "        p.grid.visible = False\n",
        "\n",
        "        show(p)\n",
        "#################################################################################################\n",
        "\n",
        "def _setup_tools():\n",
        "  \"\"\"Download and compile C++ tools.\"\"\"\n",
        "\n",
        "  # Install HHsuite\n",
        "  hhsuite_path = \"hhsuite\"\n",
        "  if not os.path.isdir(hhsuite_path):\n",
        "      print(\"Installing HHsuite...\")\n",
        "      os.makedirs(hhsuite_path, exist_ok=True)\n",
        "      url = \"https://github.com/soedinglab/hh-suite/releases/download/v3.3.0/hhsuite-3.3.0-SSE2-Linux.tar.gz\"\n",
        "      os.system(f\"curl -fsSL {url} | tar xz -C {hhsuite_path}/\")\n",
        "\n",
        "  os.environ['PATH'] += f\":{hhsuite_path}/bin:{hhsuite_path}/scripts\"\n",
        "\n",
        "\n",
        "python_version = f\"{version_info.major}.{version_info.minor}\"\n",
        "\n",
        "# this part might not be needed\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "\n",
        "# Suppress progress bars and warnings\n",
        "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\".*torch.distributed.reduce_op.*\")\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*HF_TOKEN.*\")\n",
        "\n",
        "# Install MSA Pairformer (only if not already installed)\n",
        "if not os.path.isdir(\"MSA_Pairformer\"):\n",
        "    print(\"Setting up MSA Pairformer...\")\n",
        "\n",
        "    # Capture output for git clone\n",
        "    GIT_REPO = 'https://github.com/yoakiyama/MSA_Pairformer'\n",
        "    TMP_DIR = \"tmp\"\n",
        "    os.makedirs(TMP_DIR, exist_ok=True)\n",
        "\n",
        "    result = subprocess.run(\n",
        "        f\"git clone {GIT_REPO}.git\",\n",
        "        shell=True,\n",
        "        capture_output=True,\n",
        "        text=True\n",
        "    )\n",
        "\n",
        "    # Capture pip install output\n",
        "    with io.StringIO() as buf, redirect_stdout(buf), redirect_stderr(buf):\n",
        "        subprocess.run(\n",
        "            [\"pip\", \"install\", \"-e\", \"MSA_Pairformer/\", \"--no-deps\"],\n",
        "            capture_output=True,\n",
        "            text=True\n",
        "        )\n",
        "        subprocess.run(\n",
        "            [\"pip\", \"install\", \"biopython\", \"einx\", \"jaxtyping\"],\n",
        "            capture_output=True,\n",
        "            text=True\n",
        "        )\n",
        "\n",
        "    importlib.invalidate_caches()\n",
        "    # Add the package to Python path\n",
        "    package_path = os.path.abspath(\"MSA_Pairformer\")\n",
        "    if package_path not in sys.path:\n",
        "        sys.path.insert(0, package_path)\n",
        "\n",
        "    print(\"✓ MSA Pairformer installed successfully\")\n",
        "\n",
        "# Import MSA Pairformer modules\n",
        "from MSA_Pairformer.model import MSAPairformer\n",
        "from MSA_Pairformer.dataset import MSA, aa2tok_d, prepare_msa_masks\n",
        "\n",
        "# Initialize device\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "device_name = torch.cuda.get_device_name(device) if device.type == 'cuda' else 'CPU'\n",
        "print(f\"Using device: {device_name}\")\n",
        "\n",
        "# Load model ONCE and store globally\n",
        "if 'global_model' not in globals():\n",
        "    print(\"Loading MSA Pairformer model (this will only happen once)...\")\n",
        "\n",
        "    # Suppress HuggingFace warnings during model loading\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.filterwarnings(\"ignore\")\n",
        "        # Optionally capture stdout/stderr if the model loading is still too verbose\n",
        "        with io.StringIO() as buf, redirect_stderr(buf):\n",
        "            global_model = MSAPairformer.from_pretrained(device=device).to(torch.bfloat16)\n",
        "\n",
        "    print(\"✓ Model loaded successfully and cached for reuse!\")\n",
        "else:\n",
        "    print(\"✓ Using cached model\")\n",
        "\n",
        "_setup_tools()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SjH7HTACW1IW"
      },
      "outputs": [],
      "source": [
        "# @title Settings\n",
        "#@markdown **inputs**\n",
        "sequence_a = \"PIAQIHILEGRSDEQKETLIREVSEAISRSLDAPLTSVRVIITEMAKGHFGIGGELASK\" # @param {\"type\":\"string\"}\n",
        "sequence_b = \"\" # @param {\"type\":\"string\"}\n",
        "sequence_c = \"\" # @param {\"type\":\"string\"}\n",
        "jobname = \"test\"# @param {\"type\":\"string\"}\n",
        "\n",
        "jobname = re.sub(r'\\W+', '', jobname)\n",
        "sequence = f\"{sequence_a}:{sequence_b}:{sequence_c}\"\n",
        "\n",
        "sequences, breaks = prepare_sequences(sequence)\n",
        "print(\"lengths\",[len(x) for x in sequences])\n",
        "\n",
        "#@markdown ----\n",
        "#@markdown **MSA options**\n",
        "msa_method = \"mmseqs2\" #@param [\"mmseqs2\", \"custom_fas\", \"custom_a3m\", \"custom_sto\"]\n",
        "\n",
        "#@markdown **MSA filters** (only applied when msa_method=mmseqs2)\n",
        "cov = 75 #@param [\"0\", \"25\", \"50\", \"75\", \"90\", \"99\"] {type:\"raw\"}\n",
        "qid = 15 #@param [\"0\", \"15\", \"20\", \"30\", \"40\"] {type:\"raw\"}\n",
        "#@markdown For MSA Pairformer analyses, we typically recommend starting with\n",
        "#@markdown 75% coverage (cov), and 15% minimum sequence identity with query (qid).\n",
        "\n",
        "#@markdown ----\n",
        "#@markdown **Multimer settings** (experimental option)\n",
        "neighbor_stitching = True #@param {type:\"boolean\"}\n",
        "Δgene = 1 #@param [\"0\", \"1\", \"5\", \"10\", \"20\"] {type:\"raw\"}\n",
        "#@markdown For prokaryotes, it's sometimes helpful to stitch genes based on how far part the genes are on the genome.\n",
        "\n",
        "jobname = get_unique_jobname(jobname)\n",
        "os.makedirs(jobname, exist_ok=True)\n",
        "\n",
        "msa_file = f\"{jobname}/msa.a3m\"\n",
        "if msa_method == \"mmseqs2\":\n",
        "  get_paired_msa(\n",
        "      sequences,\n",
        "      msa_file,\n",
        "      min_coverage=cov,\n",
        "      min_identity=qid,\n",
        "      genomic_distance=Δgene if neighbor_stitching else None,  # This passes the user's selected distance\n",
        "  )\n",
        "else:\n",
        "  msa_format = msa_method.split(\"_\")[1]\n",
        "  print(f\"upload {msa_method}\")\n",
        "  msa_dict = files.upload()\n",
        "  lines = []\n",
        "  for k,v in msa_dict.items():\n",
        "    lines += v.decode().splitlines()\n",
        "  input_lines = []\n",
        "  for line in lines:\n",
        "    line = line.replace(\"\\x00\",\"\")\n",
        "    if len(line) > 0 and not line.startswith('#'):\n",
        "      input_lines.append(line)\n",
        "  with open(f\"{jobname}/msa.{msa_format}\",\"w\") as msa:\n",
        "    msa.write(\"\\n\".join(input_lines))\n",
        "  if msa_format != \"a3m\":\n",
        "    os.system(f\"perl hhsuite/scripts/reformat.pl {msa_format} a3m {jobname}/msa.{msa_format} {msa_file}\")\n",
        "\n",
        "print(f\"MSA saved to: {msa_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wGsYwNc0nqmj"
      },
      "outputs": [],
      "source": [
        "#@title run MSA Pairformer\n",
        "\n",
        "mode = \"sequence_weights\" # @param [\"cb_contacts\",\"confind_contacts\",\"conservation\",\"jacobian_approx\",\"jacobian\",\"sequence_weights\", \"all\"]\n",
        "# max_msa_depth = 512 # @param [\"64\",\"128\",\"256\",\"512\",\"1024\"] {\"type\":\"raw\"}\n",
        "max_msa_depth = 512 # @param {type:\"integer\"}\n",
        "use_query_biasing = True # @param {type:\"boolean\"}\n",
        "if use_query_biasing:\n",
        "  global_model.turn_on_query_biasing()\n",
        "else:\n",
        "  global_model.turn_off_query_biasing()\n",
        "\n",
        "# Use only CERN to compute jacobian if approximating\n",
        "if mode == \"jacobian_approx\":\n",
        "    mutation_subset = [\"C\",\"E\",\"R\",\"N\"]\n",
        "    jacobian_title = \"Approximate categorical Jacobian (C, E, R, N)\"\n",
        "    mode = \"jacobian\"\n",
        "else:\n",
        "    mutation_subset = None\n",
        "    jacobian_title = \"Categorical Jacobian\"\n",
        "\n",
        "# Run MSA analyses\n",
        "results = run_msa_analysis(\n",
        "    msa_file=msa_file,\n",
        "    sequences=sequences,\n",
        "    breaks=breaks,\n",
        "    max_msa_depth=max_msa_depth,\n",
        "    mode=mode,\n",
        "    mutation_subset=mutation_subset,\n",
        "    device=device,\n",
        "    show_progress=True,\n",
        "    use_query_biasing=use_query_biasing,\n",
        "    fix_weights=True\n",
        ")\n",
        "\n",
        "# Save and plot predicted amino acid profiles\n",
        "if mode in [\"conservation\", \"all\"]:\n",
        "    # Save results\n",
        "    conservation = results['conservation']\n",
        "    np.savetxt(f\"{jobname}/conservation.txt\",conservation)\n",
        "    # Plot results\n",
        "    alphabet = \"ARNDCEQGHILKMFPSTWYV\"\n",
        "    sequence_length = conservation.shape[0]\n",
        "    idx = [str(i) for i in np.arange(1, sequence_length+1)]\n",
        "    df = pd.DataFrame(conservation, index=idx, columns=list(alphabet))\n",
        "    df = df.stack().reset_index()\n",
        "    df.columns = ['Position', 'Amino Acid', 'Probability']\n",
        "    num_colors = 256\n",
        "    palette = viridis(num_colors)\n",
        "    TOOLS=\"hover,save,pan,box_zoom,reset,wheel_zoom\"\n",
        "    p = figure(title=\"CONSERVATION\",\n",
        "              x_range=[str(x) for x in range(1, sequence_length+1)],\n",
        "              y_range=list(alphabet)[::-1],\n",
        "              width=900, height=400,\n",
        "              tools=TOOLS, toolbar_location='below',\n",
        "              tooltips=[('Position', '@Position'), ('Amino Acid', '@{Amino Acid}'), ('Probability', '@Probability')])\n",
        "\n",
        "    r = p.rect(x=\"Position\", y=\"Amino Acid\", width=1, height=1, source=df,\n",
        "              fill_color=linear_cmap('Probability', palette, low=0, high=1),\n",
        "              line_color=None)\n",
        "    p.xaxis.visible = False  # Hide the x-axis\n",
        "    show(p)\n",
        "\n",
        "# Save and plot predicted Cb-Cb contacts\n",
        "if mode in [\"cb_contacts\", \"all\"]:\n",
        "    # Save predicted confind contacts\n",
        "    cb_contacts = results[\"predicted_cb_contacts\"]\n",
        "    np.savetxt(f\"{jobname}/predicted_cb_contacts.txt\", cb_contacts)\n",
        "    # Plot predicted contacts\n",
        "    cb_analyzer = ContactAnalyzer(cb_contacts, sequences, breaks, \"Prediced Cb-Cb contacts\")\n",
        "    cb_analyzer.plot(size=512)\n",
        "\n",
        "# Save and plot predicted ConFind contacts\n",
        "if mode in [\"confind_contacts\", \"all\"]:\n",
        "    # Save predicted confind contacts\n",
        "    confind_contacts = results[\"predicted_confind_contacts\"]\n",
        "    np.savetxt(f\"{jobname}/predicted_confind_contacts.txt\", confind_contacts)\n",
        "    # Plot predicted contacts\n",
        "    confind_analyzer = ContactAnalyzer(confind_contacts, sequences, breaks, \"Predicted ConFind contacts\")\n",
        "    confind_analyzer.plot(size=512)\n",
        "\n",
        "# Save and plot categorical jacobian (or approximated version)\n",
        "if mode in [\"jacobian\", \"jacobian_approx\", \"all\"]:\n",
        "    # Save catjac results\n",
        "    jac_contacts = jac_to_con(results[\"jacobian\"], symm_before=False, symm_after=True)\n",
        "    np.savetxt(f\"{jobname}/jacobian_contacts.txt\", jac_contacts)\n",
        "    # Plot catjac results\n",
        "    catjac_analyzer = ContactAnalyzer(jac_contacts, sequences, breaks, jacobian_title)\n",
        "    catjac_analyzer.plot(size=512)\n",
        "\n",
        "if mode in [\"sequence_weights\", \"all\"]:\n",
        "    # Save sequence weights\n",
        "    with open(f\"{jobname}/sequence_weights.pkl\", \"wb\") as f:\n",
        "        pickle.dump(results[\"seq_weights_list_d\"], f)\n",
        "    # Plot median sequence weight distribution\n",
        "    mean_seq_weights_a = np.mean(np.stack([results['seq_weights_list_d'][f\"layer_{layer_idx}\"][0] for layer_idx in range(16)]), axis=0)\n",
        "    mean_seq_weights_a *= results[\"weight_scale\"]\n",
        "    f, ax = plt.subplots(1, 1, figsize=(8,4))\n",
        "    _ = ax.hist(mean_seq_weights_a, bins=50)\n",
        "    ax.axvline(x=1, linestyle='--', color='red')\n",
        "    ax.set_title(\"Sequence weight distribution\", size=18)\n",
        "    ax.set_xlabel(\"Normalized sequence weight\", size=16)\n",
        "    ax.set_ylabel(\"Count\", size=16)\n",
        "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HMx-w2QKxLw7"
      },
      "outputs": [],
      "source": [
        "#@title Show table of top predicted contacts\n",
        "from google.colab import data_table\n",
        "from IPython.display import display, Markdown\n",
        "if mode in [\"all\", \"cb_contacts\"]:\n",
        "  cb_df = cb_analyzer.get_table(min_score=None)\n",
        "  display(Markdown(\"### Top Predicted Cb-Cb Contacts\"))\n",
        "  display(data_table.DataTable(cb_df, include_index=False, num_rows_per_page=20,  min_width=10))\n",
        "\n",
        "if mode in [\"all\", \"confind_contacts\"]:\n",
        "  confind_df = confind_analyzer.get_table(min_score=None)\n",
        "  display(Markdown(\"### Top Predicted ConFind Contacts\"))\n",
        "  display(data_table.DataTable(confind_df, include_index=False, num_rows_per_page=20,  min_width=10))\n",
        "\n",
        "if mode in [\"all\", \"jacobian\", \"jacobian_approx\"]:\n",
        "  catjac_df = catjac_analyzer.get_table(min_score=None)\n",
        "  display(Markdown(\"### Top Predicted Categorical Jacobian Contacts\"))\n",
        "  display(data_table.DataTable(catjac_df, include_index=False, num_rows_per_page=20,  min_width=10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "I1Fd7a6RLeI_"
      },
      "outputs": [],
      "source": [
        "#@title download results (optional)\n",
        "from google.colab import files\n",
        "os.system(f\"zip -r {jobname}.zip {jobname}/\")\n",
        "files.download(f'{jobname}.zip')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}